{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "BeskM2NPo-AF",
      "metadata": {
        "id": "BeskM2NPo-AF"
      },
      "source": [
        "# How to Run this Notebook?\n",
        "\n",
        "This notebook builds upon the notebook for Exercise 2: RAG with Reranker. The explanations that repeat from the previous notebook are greyed out in this notebook.\n",
        "\n",
        "<font color=\"#d3d3d3\">\n",
        "1. Generate API key for OpenAI (ChatGPT): https://platform.openai.com/settings/organization/api-keys\n",
        "Make sure to save the API key. You'll get to see the key only once at the time of generation. If you miss copying the key, you may need to generate a new key.<br>\n",
        "2. Generate API token for HuggingFace Source: https://huggingface.co/settings/tokens/new?tokenType=write\n",
        "Just like last time, make sure to save this API key as well.\n",
        "<br>\n",
        "3. Generate API key for Cohere: https://dashboard.cohere.com/api-keys<br>\n",
        "4. Generate API key for LangChain to access its paid tracing tool, LangSmith: https://smith.langchain.com/settings\n",
        "  Look out for `Create API Key` on the top right side of the settings page.<br>\n",
        "</font>\n",
        "\n",
        "5. Generate API key for SerpAPI to access Google search results: https://serpapi.com/users/sign_up?plan=free\n",
        "\n",
        "6. Click on ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAAkCAIAAABXBb6UAAADK0lEQVRYCe2Xz0vrQBDH/VezS2hUgjk09FJFEX+AB8VICMaTBS+FQgXRg4egYjw09CRY8SRBkcbQYggtsbsPXBj2JbE0zYvvPegcwnR2M/PpN7MTMkf/K5v7r2jpDLfI5zVTd6YuKDB9MwRBYNu2aZr6l5mmadt2EASQughnGtwgCA4PDzHGQsIwxoZh9Hq9IlgpzT7I2u22LMsJzt8Csiy32+0iiLOpe3d3J0kSoGGMK5XKwcGBruvValUURVgSRfH6+vqPE2fAdV13aWmJASGEDMPo9/s8UBiGJycn0CSLi4uPj4/8hvz+pLifn5+7u7uMVRTF09NTQkiyPCHEsiyQeX5+XsluZ2dnycwsMilup9OBNjg6OkplZRkJIbVajf2x6a71ej0vbr1eZ7UVRXl5eWHp3t/fd3Z20Jetrq52u10W931fVdXpWAVByItLCNnb22PlNU1j0g4Gg+3tbZ5pfX0d5u7FxYWexTRNg4GTFzcMw7W1NUYGuZ6enhYWFnhcjLHjOEzgrNfUEskkE/Vuaq77+/tSqcTjCoJg23ayxiSR1BLJGyfCHQ6HW1tbjOz4+JhlSTaoLMvPz89s9ePjw/vewjBk26Io8n3f8zzXdVdWVmIPcEpcSqlpmizX8vIyNOjNzQ0ILIri5eUlKzAYDDY2NmLC8z+ho/iBAxtgdXrc29tbhJAgCBhjy7Igked5V1dXlmW9vb1B0HEcGL0AwTsAVBRuv9+vVqusZLlcdl0X4GLO6+truVzm4ZJ+4biUUsuy4AWrKMrDw0MMlFLqum6lUmF8CKHz83No4NTuBHUlSWq1WmwzdHYy/0RHjd0WRZGu6yAVxnhzc9O27W6363me4zj7+/t8D+i6HkURlOTPfq1WY2StVou9LCVJ6nQ6sPk7JwMupTSKIsMwWBMDd9JBCGmaFhOJx03eUggupXQ0GjWbTRgIycKlUqnZbI5Go5hCfweXQQRB0Gg0VFUFpRFCqqo2Gg0Yc/8QLo8ShuFwOOQjqT4hpNfrwcmLOb7v842emmGaj5/vEv1MPNtR+xmmMVVmuGPEyb00Uze3hGMSzNQdI07upV9MOsjO+LCzRgAAAABJRU5ErkJggg==) icon in the left menu bar of this Notebook\n",
        "\n",
        "7. Click `+ Add new secret `\n",
        "\n",
        "<font color=\"#d3d3d3\">\n",
        "- Add OpenAI key, if not added already:<br>\n",
        "  * Under Name copy paste: `OPENAI_KEY`<br>\n",
        "  * Under Value copy paste: OpenAI key you saved earlier<br>\n",
        "- Add HuggingFace key, if not added already:<br>\n",
        "  * Under Name copy paste: `HF_KEY`<br>\n",
        "  * Under Value copy paste: HF key you saved earlier<br>\n",
        "- Add Cohere key:<br>\n",
        "  * Under Name copy paste: `COHERE_KEY`<br>\n",
        "  * Under Value copy paste: Cohere key you saved earlier\n",
        "- Add LangChain key:\n",
        "  * Under Name copy paste: `LANG_KEY`\n",
        "  * Under Value copy paste: LangSmith key you saved earlier<br>\n",
        "  \n",
        "- Add SerpAPI key:\n",
        "  * Under Name copy paste: `SERP_KEY`\n",
        "  * Under Value copy paste: SerpAPI key you saved earlier\n",
        "\n",
        "8. Enable access to the keys for this notebook by toggling the radio buttons in the `Secrets` section.\n",
        "9. Close the `Secrets` section once done.\n",
        "10. Click `Run all` under the `Runtime` in top menu to execute this notebook`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a-jK0idCyWtt",
      "metadata": {
        "id": "a-jK0idCyWtt"
      },
      "source": [
        "#Basic Setup\n",
        "<font color=\"#d3d3d3\">\n",
        "**Install Frameworks**<br><br>\n",
        "`langchain`, `langchain_core`, and `langchain_community` are components of the LangChain framework.<br><br>\n",
        "`faiss-cpu` is Facebook AI's similarity search library, optimized for CPU usage.<br><br>\n",
        "`openai==1.56.2` and `langchain_openai` are OpenAI's Python client library and LangChain's OpenAI integration package respectively.<br><br>\n",
        "`Cohere` and `langchain_Cohere` are Cohere's Python client library and LangChain's Cohere integration package respectively.\n",
        "\n",
        "`google-search-results`is used for retrieving search results from Google, potentially for augmenting the information used by the language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8f8207",
      "metadata": {
        "id": "5e8f8207"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_core langchain_community faiss-cpu openai==1.56.2 langchain_openai cohere langchain_cohere google-search-results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uq9Q1b21Ufk-",
      "metadata": {
        "id": "uq9Q1b21Ufk-"
      },
      "source": [
        "**Import Packages**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k1XadP0546aj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1XadP0546aj",
        "outputId": "06c5f41e-6933-4341-b9d7-afa0aa774512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import requests\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "import re\n",
        "\n",
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain_core.tools import BaseTool\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.agents import create_react_agent\n",
        "from langchain.agents import AgentExecutor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "082f85f0",
      "metadata": {
        "id": "082f85f0"
      },
      "source": [
        "**Project's Secret Keys Setup**\n",
        "\n",
        "<font color=\"#d3d3d3\">\n",
        "This code sets up authentication credentials by retrieving API keys from Colab's `Secrets` storage.\n",
        "<br>\n",
        "Cohere's and LangChain's API key are also retreieved from Colab's `Secrets` storage.</font><br>\n",
        "SERPApi key is also retrieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da18250",
      "metadata": {
        "id": "1da18250"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_KEY')\n",
        "hfToken = userdata.get('HF_KEY')\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_KEY')\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANG_KEY')\n",
        "os.environ[\"SERPAPI_API_KEY\"] = userdata.get('SERP_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b76U-mEe1RDT",
      "metadata": {
        "id": "b76U-mEe1RDT"
      },
      "source": [
        "# **LangChain Tracing Configuration by LangSmith**\n",
        "\n",
        "<font color=\"#d3d3d3\">This code configures LangChain to use LangSmith for tracing. LangSmith is a tool that helps you monitor and debug your LangChain applications. Tracing means that LangSmith will keep a record of every step your application takes, allowing you to see how it's working and identify any issues.<br>\n",
        "-<font color=\"#d3d3d3\"> `os.environ` is a way to access and modify environment variables in Python. Environment variables are like global settings for your system.<br>\n",
        "-`LANGCHAIN_TRACING_V2` controls LangChain tracing. By setting it to \"true\", we are enabling LangChain tracing. This means that LangChain will start sending data to LangSmith.\n",
        "\n",
        "-`LANGCHAIN_PROJECT`  specifies the name of the LangSmith project to use. In this example, `10_K_React` is the name of the project. You can create a new LangSmith project or use an existing one. Projects are a way to organize your tracing data in LangSmith."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fIUSBo4p1Fg9",
      "metadata": {
        "id": "fIUSBo4p1Fg9"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"10_K_React\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2k78A3s4ExZq",
      "metadata": {
        "id": "2k78A3s4ExZq"
      },
      "source": [
        "# Configuration Dictionary\n",
        "<font color=\"#d3d3d3\">\n",
        "Contains settings for document processing and RAG system, including parameters for text chunking, embedding generation, document retrieval, model specifications, prompts used and API endpoints.\n",
        "Default config settings for Cohere's Reranker were also added.</font><br>\n",
        "The keys needed for ReAct were added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trX6Tl23lyWq",
      "metadata": {
        "id": "trX6Tl23lyWq"
      },
      "outputs": [],
      "source": [
        "defaultConfig = {\n",
        "    # Document processing settings\n",
        "    \"chunkSize\": 500,\n",
        "    \"chunkOverlap\": 50,\n",
        "    \"userAgentHeader\": \"YourCompany-ResearchBot/1.0 (your@email.com)\",\n",
        "\n",
        "    # Embedding settings\n",
        "    \"embeddingApiUrl\": \"https://api-inference.huggingface.co/models/BAAI/bge-base-en-v1.5\",\n",
        "    \"embeddingDim\": 768,\n",
        "\n",
        "    #Defalt document\n",
        "    \"defaultDocument\" : Document(\n",
        "        page_content = \"This is a default document.\",\n",
        "        metadata = {\"source\": \"default\"}\n",
        "    ),\n",
        "\n",
        "    # Vector store settings\n",
        "    \"numRetrievedDocuments\": 12,\n",
        "\n",
        "    # Document formatter settings\n",
        "    \"numSelectedDocuments\": 5,\n",
        "\n",
        "    #Reranker setting\n",
        "    \"rerankerModel\": \"rerank-english-v3.0\",\n",
        "    \"numRerankedDocuments\": 5,\n",
        "\n",
        "    # Model settings\n",
        "    \"ragAnswerModel\": \"gpt-4o\",\n",
        "    \"ragAnswerModelTemeprature\": 0.7,\n",
        "\n",
        "    # URLs to process\n",
        "    \"companyFilingUrls\": [\n",
        "        (\"Tesla\", \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\"),\n",
        "        (\"General Motors\", \"https://www.sec.gov/Archives/edgar/data/1467858/000146785824000031/gm-20231231.htm\")\n",
        "    ],\n",
        "\n",
        "    # RAG prompt template\n",
        "    \"ragPromptTemplate\": \"\"\"\n",
        "    Give an answer for the `Question` using only the given `Context`. Use information relevant to the query from the entire context.\n",
        "    Provide a detailed answer with thorough explanations, avoiding summaries.\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\",\n",
        "\n",
        "    #Query Decomposer settings\n",
        "    \"numQueryDecomposerSelectedDocuments\": 2,\n",
        "    \"queryDecomposerModel\": \"gpt-4o-mini\",\n",
        "    \"queryDecomposerModelTemperature\": 0.8,\n",
        "\n",
        "    \"subqueryPromptTemplate\": \"\"\"\n",
        "    Break down the `Question` into multiple sub-queries. Use the guidelines given below to help in the task.\n",
        "\n",
        "    1. The set of sub-queries together capture the complete information needed to answer the question.\n",
        "    2. Each sub-query should ask for just one piece of information about one specific company.\n",
        "    3. For each sub-query, only mention the information you're trying to get. Don't use verbs like \"retrieve\" or \"find\".\n",
        "    4. Include the company name mentioned in each sub-query.\n",
        "    5. Do not include any references to data sources in your sub-queries.\n",
        "\n",
        "    Enclose the sub-query in angle brackets. For example:\n",
        "    <sub-query 1>\n",
        "    <sub-query 2>\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Begin:\n",
        "    \"\"\",\n",
        "\n",
        "\n",
        "    # Settings for ReAct\n",
        "    \"reactModelName\": \"gpt-4o\",\n",
        "    \"reactModelTemperature\": 0,\n",
        "\n",
        "    \"reactPromptTemplate\": \"\"\"Your task is to gather relevant information to build context for the question. Focus on collecting details related to the question.\n",
        "    Gather as much context as possible before formulating your answer.\n",
        "\n",
        "    You have access to the following tools:\n",
        "\n",
        "    {tools}\n",
        "\n",
        "    Use the following format:\n",
        "\n",
        "    Question: the input question you must answer\n",
        "\n",
        "    Thought: you should always think about what to do\n",
        "\n",
        "    Action: the action to take, should be one of [{tool_names}]\n",
        "\n",
        "    Action Input: the input to the action\n",
        "\n",
        "    Observation: the result of the action\n",
        "\n",
        "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "\n",
        "    Thought: I now know the final answer\n",
        "\n",
        "    Final Answer: the final answer to the question.\n",
        "\n",
        "    Follow these steps:\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Question: {input}\n",
        "\n",
        "    Thought:{agent_scratchpad}\n",
        "    \"\"\",\n",
        "\n",
        "    \"reactVerbosity\": True,\n",
        "\n",
        "\n",
        "    # Settings for Extracting Director's Name\n",
        "    \"nameExtractionModel\": \"gpt-4o-mini\",\n",
        "    \"nameExtractionModelTemperature\": 0.4,\n",
        "    \"nameExtractionPrompt\": \"\"\"\n",
        "    Extract and list the names of all individuals with the title 'Director' from the following text, excluding any additional information such as dates or signatures.\n",
        "    Present the names as a simple, comma-separated list.\n",
        "\n",
        "    {text}\n",
        "    \"\"\",\n",
        "\n",
        "    # Director Information Tool settings\n",
        "    \"useDirectorTool\": True,\n",
        "    \"directorToolName\": \"Company Directors Information\",\n",
        "    \"directorToolDescription\": \"Retrieve the names of company directors for a chosen company. Optionally, their LinkedIn handles can also be included. Use the format: company_name, true/false.\",\n",
        "\n",
        "    # Web Search Tool settings\n",
        "    \"useWebTool\": True,\n",
        "    \"webToolName\": \"WebSearch\",\n",
        "    \"webToolDescription\": \"Performs a web search on the query.\",\n",
        "    \"numWebToolResults\": 3,\n",
        "\n",
        "    # Database Retriever Tool settings\n",
        "    \"useRetrieverTool\": True,\n",
        "    \"retrieverToolName\": \"Vector Reranker Search\",\n",
        "    \"retrieverToolDescription\": \"Retrieves information from an embedding based vector DB containing financial data and company information. Structure query as a sentence\",\n",
        "    \"numRetrieverToolResults\": 3\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LjODOSh_qNDo",
      "metadata": {
        "id": "LjODOSh_qNDo"
      },
      "source": [
        "# Preprocessing Class\n",
        "<font color=\"#d3d3d3\">`FilingsPreProcessor` (Custom-defined class):\n",
        "A custom class that downloads and preprocesses company filing documents from the web, splitting large filings into smaller text chunks for easier embedding and retrieval.<br>\n",
        "`__init __`:\n",
        "Constructor method that initializes the preprocessor with configuration parameters for chunk size, overlap, and an internal dictionary to store text chunks used specificially for extracting director names.<br>\n",
        "`loadAndProcessFilings`:\n",
        "Method that takes company filing URLs, downloads content, processes it into chunks, and stores the last 1000 characters for director names; returns processed documents and director name chunks.\n",
        "\n",
        "- <font color=\"#d3d3d3\">`Document` (Langchain class):\n",
        "LangChain class representing a document with content and metadata, which is company name in this exercise.\n",
        "\n",
        "- <font color=\"#d3d3d3\">`WebBaseLoader` (Langchain class):\n",
        "  * <font color=\"#d3d3d3\">LangChain class that downloads content from the given 10-K company filing URLs with a custom User-Agent Header, which informs SEC website where the request is coming from and what kind of user agent it is. It's like introducing yourself when you enter a room, but for computer programs! This is often required for accessing web data, especially in responsible web scraping.\n",
        "\n",
        "- <font color=\"#d3d3d3\">`RecursiveCharacterTextSplitter` (Langchain class):\n",
        "LangChain class that splits text documents into smaller chunks with specified size and overlap. It adds the company names as metadata for each chunk, then returns the processed chunks.\n",
        "</font>\n",
        "\n",
        "`directorNamesChunk` stores the last 1000 characters dedicated to extracting director names. This information is crucial for the CompanyDirectorsTool used later in the code.Addition of this feature enhances the capability of the system to extract and utilize director information, which can be valuable for answering complex queries about company leadership and governance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cTPHkr2ssOMH",
      "metadata": {
        "id": "cTPHkr2ssOMH"
      },
      "outputs": [],
      "source": [
        "class FilingsPreProcessor:\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.chunkSize = config[\"chunkSize\"]\n",
        "        self.chunkOverlap = config[\"chunkOverlap\"]\n",
        "        self.userAgentHeader = config[\"userAgentHeader\"]\n",
        "        self.directorNamesChunk = {}\n",
        "\n",
        "    # Using langchain class Document\n",
        "    def loadAndProcessFilings(self, companyFilingUrls: List[Tuple[str, str]]) -> List[Document]:\n",
        "        processedCompanyFilings = []\n",
        "\n",
        "        for company, url in companyFilingUrls:\n",
        "            try:\n",
        "                # Using langchain class WebBaseLoader\n",
        "                filingContent = WebBaseLoader(url, header_template = {'User-Agent': self.userAgentHeader}).load()\n",
        "\n",
        "                self.directorNamesChunk[company] = filingContent[0].page_content[-1000:]\n",
        "\n",
        "                # Using langchain class RecursiveCharacterTextSplitter\n",
        "                splitFilingContent = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size = self.chunkSize,\n",
        "                    chunk_overlap = self.chunkOverlap\n",
        "                    ).transform_documents(filingContent)\n",
        "\n",
        "                for split in splitFilingContent:\n",
        "                    split.metadata.update({\n",
        "                        'company': company,\n",
        "                    })\n",
        "\n",
        "                processedCompanyFilings.extend(splitFilingContent)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return processedCompanyFilings, self.directorNamesChunk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0nutwjAuqTC4",
      "metadata": {
        "id": "0nutwjAuqTC4"
      },
      "source": [
        "# Embedding class\n",
        "<font color=\"#d3d3d3\">`EmbeddingService` (Custom-defined class):\n",
        "A custom class that inherits from LangChain's Embeddings class, providing text embedding functionality through an external API<br>\n",
        "`__init __`:\n",
        "Constructor that initializes the service with API URL, headers, and embedding configuration\n",
        "- <font color=\"#d3d3d3\">Sets up the Hugging Face embedding API endpoint an headers\n",
        "- Stores an instruction prefix that is added to user's query before embedding generation.\n",
        "- Sets the embedding dimension size as per `embeddingDim` value in `defaultConfig` (e.g., 768)\n",
        "\n",
        "<font color=\"#d3d3d3\">`embed_documents`:\n",
        "From LangChain's interface - transforms a list of texts into embeddings by removing newlines and calling _generateEmbeddings\n",
        "\n",
        "<font color=\"#d3d3d3\">`embed_query`:\n",
        "From LangChain's interface - transforms a single query text into an embedding by adding instruction prefix and removing newlines\n",
        "\n",
        "<font color=\"#d3d3d3\">`_generateEmbeddings`:\n",
        "Internal helper method that:\n",
        "- <font color=\"#d3d3d3\">Splits the input texts into batches (default size 30)\n",
        "- Sends each batch to the embedding API with _sendBatchRequest\n",
        "- Handles retries and times out if requests take too long\n",
        "- Ensures each embedding matches the expected dimension (e.g., 768)\n",
        "- Returns a list of embeddings\n",
        "\n",
        "<font color=\"#d3d3d3\">`_sendBatchRequest`:\n",
        "Internal helper method that makes POST requests to the embedding API and handles the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hLSnWAK5xBCl",
      "metadata": {
        "id": "hLSnWAK5xBCl"
      },
      "outputs": [],
      "source": [
        "class EmbeddingService(Embeddings):               # Using langchain class Embeddings\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.api_url = config[\"embeddingApiUrl\"]\n",
        "        self.headers = {\"Authorization\": f\"Bearer {hfToken}\"}\n",
        "        self.queryInstruction = \"Represent this question for searching relevant passages: \"\n",
        "        self.embeddingDim = config[\"embeddingDim\"]\n",
        "\n",
        "    def embed_documents(self, texts: list[str]) -> List[List[float]]:\n",
        "        textsToEmbed = [text.replace(\"\\n\", \" \") for text in texts]\n",
        "        return self._generateEmbeddings(textsToEmbed)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        textToEmbed = self.queryInstruction + text.replace(\"\\n\", \" \")\n",
        "        return self._generateEmbeddings([textToEmbed])[0]\n",
        "\n",
        "    def _generateEmbeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        results = [np.zeros(self.embeddingDim).tolist() for _ in range(len(texts))]\n",
        "        batchSize = 30\n",
        "        longRetries = 0\n",
        "\n",
        "        batches = [(i, texts[i:i + batchSize]) for i in range(0, len(texts), batchSize)]\n",
        "\n",
        "        for Id, batch in batches:\n",
        "            startTime = time.time()\n",
        "\n",
        "            while time.time() - startTime < 30:\n",
        "                try:\n",
        "                    response = self._sendBatchRequest(batch)\n",
        "\n",
        "                    if not isinstance(response, list):\n",
        "                        continue\n",
        "\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    time.sleep(1)\n",
        "                    continue\n",
        "\n",
        "            if time.time() - startTime > 10:\n",
        "                longRetries += 1\n",
        "                if longRetries > 3:\n",
        "                    print(\"Too many long retries, stopping embedding generation\")\n",
        "                    return results\n",
        "\n",
        "            for j, embedding in enumerate(response):\n",
        "                if isinstance(embedding, list) and len(embedding) == self.embeddingDim:\n",
        "                    results[Id + j] = embedding\n",
        "                else:\n",
        "                    print(f\"Invalid embedding format at index {Id + j}: {embedding[:100]}...\")\n",
        "\n",
        "        if len(texts) > 1:\n",
        "            successful = sum(1 for emb in results if any(emb))\n",
        "            print(f\"Successfully embedded {successful}/{len(texts)} texts\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _sendBatchRequest(self, batch: List[str]) -> List[List[float]]:\n",
        "        response = requests.post(\n",
        "            self.api_url,\n",
        "            headers = self.headers,\n",
        "            json = {\"inputs\": batch}\n",
        "        )\n",
        "        if response.status_code != 200:\n",
        "            raise ValueError(f\"API returned status code {response.status_code}\")\n",
        "        return response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ksNBSQs0qWyf",
      "metadata": {
        "id": "ksNBSQs0qWyf"
      },
      "source": [
        "# Vector DB class\n",
        "<font color=\"#d3d3d3\">`VectorStore` (Custom Class):\n",
        "This custom class manages a FAISS-based vector store for storing and retrieving them by cosine similarity.\n",
        "\n",
        "<font color=\"#d3d3d3\">`__init __`:\n",
        "Constructor that initializes a FAISS vector store with a default document and configures a retriever with number of documents to retrieve.\n",
        "- <font color=\"#d3d3d3\">`FAISS` (Langchain class):\n",
        "A LangChain wrapper class for Facebook AI Similarity Search library. It manages vector database creation, indexes embeddings and retrieves embeddings similar to the queried embedding. Its `from_documents` function accepts a chunk/query, and an instance of `Embedding` Langchain class for generating embeddings for provided chunk/query.\n",
        "- The VectorStore class creates a retriever using self.store.as_retriever(). This retriever is responsible for fetching relevant documents from the vector store based on a query's embedding.\n",
        "\n",
        "<font color=\"#d3d3d3\">`addDocuments`:\n",
        "It adds new documents to the FAISS vector store created in `__init__`, accepting a list of Document objects as input\n",
        "- <font color=\"#d3d3d3\">`Document` (Langchain class):\n",
        " A LangChain document class is used to represent each chunk as a `Document` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hEg0P2yA0FWA",
      "metadata": {
        "id": "hEg0P2yA0FWA"
      },
      "outputs": [],
      "source": [
        "class VectorStore:\n",
        "    def __init__(self, config, embeddingFunction):\n",
        "        # Using langchain class FAISS\n",
        "        self.store = FAISS.from_documents([config[\"defaultDocument\"]], embeddingFunction)\n",
        "        self.retriever = self.store.as_retriever(\n",
        "            search_kwargs = {\"k\": config[\"numRetrievedDocuments\"]}\n",
        "        )\n",
        "\n",
        "    def addDocuments(self,  documents: List[Document]) -> None:\n",
        "        self.store.add_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MLWpwaAV6Amd",
      "metadata": {
        "id": "MLWpwaAV6Amd"
      },
      "source": [
        "# Reranker Function\n",
        "<font color=\"#d3d3d3\">It enhances the quality of search results by re-ranking them using Cohere's reranking capabilities within the LangChain framework.\n",
        "\n",
        "<font color=\"#d3d3d3\">`createReranker`:\n",
        "The first parameter in this function is the name of Cohere's Model that we want to use and the default model name is obtained from `config`. `numRerankedDocuments` defines number of top-ranked chunks to keep after reranking.\n",
        "\n",
        "<font color=\"#d3d3d3\">- `CohereRerank` (Langchain class):\n",
        "LangChain class specifically for reranking chunks using Cohere's reranking API. It reorders chunks based on their relevance to a query using Cohere's model\n",
        "\n",
        "<font color=\"#d3d3d3\">- `ContextualCompressionRetriever` (Langchain class):\n",
        "It sets the previously created CohereRerank instance, `compressor`, as `base_compressor`. It also requires a `base_retriever` which is the retriever object retruned from vector database retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E8rVJiaH5_lE",
      "metadata": {
        "id": "E8rVJiaH5_lE"
      },
      "outputs": [],
      "source": [
        "def createReranker(config, baseRetriever):\n",
        "    # Using langchain class CohereRerank\n",
        "    compressor = CohereRerank(model=config[\"rerankerModel\"], top_n = config[\"numRerankedDocuments\"])\n",
        "\n",
        "    # Using langchain class ContextualCompressionRetriever\n",
        "    return ContextualCompressionRetriever(\n",
        "        base_compressor = compressor, base_retriever = baseRetriever\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EqV92IQJ6_q2",
      "metadata": {
        "id": "EqV92IQJ6_q2"
      },
      "source": [
        "# Query Decomposer Class\n",
        "<font color=\"#d3d3d3\">`QueryDecomposer` (Custom Class): This class is designed to take a complex user question and break it down into smaller, more focused sub-queries. This is helpful for improving the accuracy and efficiency of retrieving relevant information from a large document.\n",
        "\n",
        "<font color=\"#d3d3d3\">`__init__`: Constructor that initializes the decomposer with configuration parameters:<br>- `ChatOpenAI` (Langchain class): It initializes a Language Model (LLM) from OpenAI, specifically using the model and temperature defined in the config dictionary. This LLM will be responsible for actually breaking down the question.<br>- `PromptTemplate` (Langchain class): This creates a prompt template using the instructions provided in `config[\"subqueryPromptTemplate\"]`. This template guides the LLM on how to decompose the question, like specifying the format for the sub-queries.\n",
        "\n",
        "\n",
        "<font color=\"#d3d3d3\">`_createChain`: Internal helper method that:<br>- Constructs a processing pipeline using LangChain's pipe operator (|)<br>- It connects three steps:<br>  * self.prompt: The prompt template created earlier is fed into the chain first.<br>  * self.llm: The prompt is then passed to the OpenAI LLM to generate the sub-queries.<br>  * StrOutputParser(): Finally, the LLM's output is parsed into a clean string format using StrOutputParser.\n",
        "\n",
        "<font color=\"#d3d3d3\">`decomposeQuery`:<br>- Primary interface method that processes a question and returns decomposed sub-queries<br>- Uses regex pattern matching to extract sub-queries text between angle brackets (<...>) and return them as a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lq9tvNSa6-eM",
      "metadata": {
        "id": "Lq9tvNSa6-eM"
      },
      "outputs": [],
      "source": [
        "class QueryDecomposer:\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.llm = ChatOpenAI(model=config[\"queryDecomposerModel\"], temperature=config[\"queryDecomposerModelTemperature\"])\n",
        "        self.prompt = PromptTemplate.from_template(config[\"subqueryPromptTemplate\"])\n",
        "        self.chain = self._createChain()\n",
        "\n",
        "    def _createChain(self):\n",
        "        return (\n",
        "            self.prompt\n",
        "            | self.llm                      # Using langchain class StrOutputParser\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "    def decomposeQuery(self, question: str) -> List[str]:\n",
        "        response = self.chain.invoke(question)\n",
        "        return re.findall(r'<(.*?)>', response, re.DOTALL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3qAknYQO6aLQ",
      "metadata": {
        "id": "3qAknYQO6aLQ"
      },
      "source": [
        "# Company Directors Linkedin Handle Tool\n",
        "`CompanyDirectorsTool`(Custom-defined class): A custom tool class that inherits from LangChain's BaseTool, designed to extract and process company director information.\n",
        "\n",
        "`__init__`: Constructor that initializes the agent with the RAG chain, necessary tools, and an agent executor.\n",
        "- It sets up the ReAct agent with the specified model and tools, connecting it to the RAG pipeline for context retrieval and response generation.\n",
        "- `create_react_agent` (Langchain function): Initializes a ReAct agent using the specified language model, tools, and prompt template. This agent can dynamically decide which tools to use to gather information to answer a question.\n",
        "- `AgentExecutor` (Langchain class): A Langchain class responsible for executing the actions determined by the agent. It manages the interaction between the agent and the tools.\n",
        "- `vBaseTool` (Langchain class): Base class for custom tools in LangChain's agent framework providing the structure and basic functionalities.\n",
        "- `SerpAPIWrapper` (Langchain class): LangChain wrapper for using SerpAPI to interact with search engines and retrieve web search results. This class would be used by tools (e.g., for web searching capabilities).\n",
        "- `CommaSeparatedListOutputParser` (Langchain class): A LangChain class that converts output into a list by splitting it into items wherever commas are present.\n",
        "\n",
        "`_run`:\n",
        "- This method is the core logic of the `CompanyDirectorsTool`. It's executed when the agent decides to use this tool.\n",
        "- It takes a query string as input, which should be in the format \"company_name, true/false\" to indicate whether to include LinkedIn handles.\n",
        "- It first extracts the company name and the LinkedIn handle inclusion flag from the query.\n",
        "- It then retrieves the relevant snippet of director information from the `directorstringDict` using the company name.\n",
        "- If LinkedIn handles are requested, it iterates through the director names and calls `_getLinkedinHandle` to fetch them.\n",
        "- Finally, it returns a formatted string containing the director names and their LinkedIn handles (if requested).\n",
        "\n",
        "\n",
        "`_getLinkedinHandle`:\n",
        "- This method is responsible for retrieving the LinkedIn handle of a director given their name and company.\n",
        "- It first checks if the LinkedIn handle is already cached in `linkedinCache`. If so, it returns the cached value.\n",
        "- If not cached, it uses the `SerpAPIWrapper` to perform a web search on LinkedIn for the director's profile.\n",
        "- It extracts the LinkedIn profile URL from the search results.\n",
        "- It caches the LinkedIn handle for future use.\n",
        "- It returns the LinkedIn handle (or an error message if not found)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SWLboROY6qwm",
      "metadata": {
        "id": "SWLboROY6qwm"
      },
      "outputs": [],
      "source": [
        "linkedinCache = {}\n",
        "class CompanyDirectorsTool(BaseTool):       # Inherits Langchain class BaseTool\n",
        "    name: str\n",
        "    description: str\n",
        "    directorstringDict: dict\n",
        "    serpApiObj: Any\n",
        "\n",
        "    llm: Any\n",
        "    parser: Any\n",
        "    nameExtractionPrompt: Any\n",
        "    nameExtractionChain: Any\n",
        "\n",
        "    def __init__(self, config, directorstringDict):\n",
        "        availableCompanies = directorstringDict.keys()\n",
        "\n",
        "        super().__init__(\n",
        "            name = config[\"directorToolName\"],\n",
        "            description = f\"{config['directorToolDescription']} Available companies: {', '.join(availableCompanies)}\",\n",
        "            directorstringDict = directorstringDict,\n",
        "            # Using langchain class SerpAPIWrapper\n",
        "            serpApiObj = SerpAPIWrapper(),\n",
        "\n",
        "            llm = ChatOpenAI(model=config[\"nameExtractionModel\"], temperature=config[\"nameExtractionModelTemperature\"]),\n",
        "            # Using langchain class CommaSeparatedListOutputParser\n",
        "            parser = CommaSeparatedListOutputParser(),\n",
        "            nameExtractionPrompt = PromptTemplate.from_template(config[\"nameExtractionPrompt\"]),\n",
        "            nameExtractionChain = None\n",
        "        )\n",
        "        self.nameExtractionChain = self.nameExtractionPrompt | self.llm | self.parser\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        companyName, includeLinkedin = query.split(',')\n",
        "        companyName = companyName.strip()\n",
        "        includeLinkedin = True #includeLinkedin.strip().lower() == 'true'\n",
        "        companySnippet = self.directorstringDict.get(companyName)\n",
        "\n",
        "        if not companySnippet:\n",
        "            return f\"No director information found for {companyName}\"\n",
        "\n",
        "        directorNames = self.nameExtractionChain.invoke({\"text\": companySnippet})\n",
        "\n",
        "        if not includeLinkedin:\n",
        "            return f\"Directors of {companyName}: {', '.join(directorNames)}\"\n",
        "\n",
        "        directorHandles = []\n",
        "        for name in directorNames:\n",
        "            linkedinHandle = self._getLinkedinHandle(name, companyName)\n",
        "            directorHandles.append(f\"{name} (LinkedIn: {linkedinHandle})\")\n",
        "\n",
        "        return f\"Directors of {companyName}: {'; '.join(directorHandles)}\"\n",
        "\n",
        "    def _getLinkedinHandle(self, name: str, company: str) -> str:\n",
        "\n",
        "        cacheKey = f\"{name}_{company}\"\n",
        "        if cacheKey in linkedinCache:\n",
        "            return linkedinCache[cacheKey]\n",
        "        try:\n",
        "            results = self.serpApiObj.results(f'\"{name}\" {company} site:linkedin.com/in/')\n",
        "            link = results.get(\"organic_results\", [{}])[0].get(\"link\", \"Profile not found\")\n",
        "            linkedinCache[cacheKey] = link\n",
        "            return link\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error finding LinkedIn profile: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R5i5izmmS20s",
      "metadata": {
        "id": "R5i5izmmS20s"
      },
      "source": [
        "# WebSearch Tool\n",
        "\n",
        "WebSearchTool (Custom-defined class): A custom tool class that inherits from LangChain's BaseTool, providing web search functionality using the SerpAPI service\n",
        "\n",
        "BaseTool (Langchain class): LangChain class that serves as a base for building custom tools in a LangChain agent framework.\n",
        "__init __ (Custom method): Constructor that initializes the web search tool with configuration parameters and creates a SerpAPIWrapper instance from the LangChain library\n",
        "\n",
        "SerpAPIWrapper (Langchain class): LangChain class for integrating SerpAPI to fetch and process search engine results.\n",
        "_run (Custom method): Core method that executes the web search query using SerpAPI and returns formatted results, handling any potential errors during the search process\n",
        "\n",
        "_formatResults (Custom method): Helper method that processes and formats the raw search results into a readable string, extracting title, snippet, and link for each result up to the specified number of results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hcrkL9NDS3pp",
      "metadata": {
        "id": "hcrkL9NDS3pp"
      },
      "outputs": [],
      "source": [
        "class WebSearchTool(BaseTool):              # Using langchain class BaseTool\n",
        "    name: str\n",
        "    description: str\n",
        "    serpApi: Any\n",
        "    numResults: int\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(\n",
        "            name = config[\"webToolName\"],\n",
        "            description = config[\"webToolDescription\"],\n",
        "            # Using langchain class SerpAPIWrapper\n",
        "            serpApi = SerpAPIWrapper(),\n",
        "            numResults = config[\"numWebToolResults\"]\n",
        "        )\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        try:\n",
        "            results = self.serpApi.results(query)\n",
        "            return self._formatResults(results)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error performing web search: {str(e)}\"\n",
        "\n",
        "    def _formatResults(self, results: Dict) -> str:\n",
        "        formatted = []\n",
        "\n",
        "        for result in results.get(\"organic_results\", [])[:self.numResults]:\n",
        "            formatted.append(\n",
        "                f\"Title: {result.get('title', 'N/A')}\\n\"\n",
        "                f\"Snippet: {result.get('snippet', 'N/A')}\\n\"\n",
        "                f\"Link: {result.get('link', 'N/A')}\"\n",
        "            )\n",
        "        return \"\\n\\n\".join(formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_-kjBfhyJZ2c",
      "metadata": {
        "id": "_-kjBfhyJZ2c"
      },
      "source": [
        "# Vector Reranker Tool\n",
        "`VectorRerankerSearchTool` (Custom-defined class):\n",
        "A custom tool class that inherits from LangChain's BaseTool, designed to perform document retrieval based on queries and return formatted results\n",
        "\n",
        "- `BaseTool` (Langchain class):\n",
        "LangChain class that serves as a base for building custom tools in a LangChain agent framework.\n",
        "\n",
        "`__init __` (Custom method):\n",
        "Constructor method that initializes the tool with configuration parameters and a retriever object, setting up the tool's name, description, retriever instance, and number of results to return\n",
        "\n",
        "`_run` (Custom method):\n",
        "Core execution method that processes a query using the retriever, returns concatenated page content from retrieved documents up to the specified limit, and includes error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8UQ2_1zAJQZP",
      "metadata": {
        "id": "8UQ2_1zAJQZP"
      },
      "outputs": [],
      "source": [
        "class VectorRerankerSearchTool(BaseTool):      # Using langchain class BaseTool\n",
        "    name: str\n",
        "    description: str\n",
        "    retriever: Any\n",
        "    numResults: int\n",
        "\n",
        "    def __init__(self, config, retriever):\n",
        "        super().__init__(\n",
        "            name = config[\"retrieverToolName\"],\n",
        "            description = config[\"retrieverToolDescription\"],\n",
        "            retriever = retriever,\n",
        "            numResults = config[\"numRetrieverToolResults\"]\n",
        "        )\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        try:\n",
        "            docs = self.retriever.invoke(query)\n",
        "            return \"\\n\\n\".join(f\"{doc.page_content}\" for doc in docs[:self.numResults])\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving documents: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lmChffd7JyPb",
      "metadata": {
        "id": "lmChffd7JyPb"
      },
      "source": [
        "# React class\n",
        "`React` (Custom-defined class):\n",
        " A custom class that manages the initialization and execution of a ReAct (Reasoning and Acting) agent with various tools and configurations\n",
        "\n",
        "`__init __` (Custom method):\n",
        " Constructor that initializes the React class with configuration settings and director information\n",
        " - `config`: This is a dictionary holding settings like what language model to use, what tools are available, etc. It's like the instruction manual for the agent.\n",
        " - `self.agent`: This will store the actual ReAct agent once it's created. It starts as None because it hasn't been built yet.\n",
        " - `self.reactAgentExecutor`: This will handle running the agent and its tools. It also starts as None.\n",
        "\n",
        "`_initializeTools` (Custom method):\n",
        " Private method that sets up different tools (company directors, web search, retriever) based on configuration flags\n",
        " - `CompanyDirectorsTool`: Provides information about company directors (even their LinkedIn profiles).\n",
        " - `WebSearchTool`: Lets the agent search the web.\n",
        " - `VectorRerankerSearchTool`: Searches through the database using the retriever.\n",
        "\n",
        "`initializeAgent` (Custom method):\n",
        " Configures the ReAct agent with specified tools, prompt template, and language model. This method actually creates the ReAct agent and sets it up to be used. It calls _initializeTools to get the list of tools.\n",
        " - create_react_agent: This LangChain function builds the ReAct agent, giving it the language model (llm), tools, and instructions (prompt).\n",
        "\n",
        "- `ChatOpenAI` (Langchain class):\n",
        "LangChain class for interfacing with OpenAI's chat models\n",
        "\n",
        "- `PromptTemplate` (Langchain class):\n",
        "LangChain class for creating structured prompts\n",
        "\n",
        "- `AgentExecutor` (Langchain class):\n",
        " LangChain class that handles the execution of agents with their associated tools\n",
        "\n",
        "`run` (Custom method):\n",
        " Executes the initialized ReAct agent with a given question and returns the response\n",
        " - `self.reactAgentExecutor`: This refers to the instance of the AgentExecutor class associated with the current React object. It's the component responsible for actually running the ReAct agent and its tools.\n",
        "  - `\"input\": question`: This part of the dictionary specifies the user's question that the agent needs to answer. The question variable holds the actual question string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bko8ME6gnFxm",
      "metadata": {
        "id": "bko8ME6gnFxm"
      },
      "outputs": [],
      "source": [
        "class React:\n",
        "    def __init__(self, config: Dict[str, Any], directorStrings):\n",
        "        self.config = config\n",
        "        self.directorStrings = directorStrings\n",
        "        self.agent = None\n",
        "        self.reactAgentExecutor = None\n",
        "\n",
        "    def _initializeTools(self, retriever):\n",
        "        self.tools = []\n",
        "\n",
        "        if self.config.get(\"useDirectorTool\", False):\n",
        "            self.tools.append(CompanyDirectorsTool(self.config, self.directorStrings))\n",
        "\n",
        "        if self.config.get(\"useWebTool\", False):\n",
        "            self.tools.append(WebSearchTool(self.config))\n",
        "\n",
        "        if self.config.get(\"useRetrieverTool\", False):\n",
        "            self.tools.append(VectorRerankerSearchTool(self.config, retriever))\n",
        "\n",
        "    def initializeAgent(self, retriever):\n",
        "        self._initializeTools(retriever)\n",
        "        prompt = PromptTemplate.from_template(self.config[\"reactPromptTemplate\"])\n",
        "        llm = ChatOpenAI(model=self.config[\"reactModelName\"], temperature = self.config[\"reactModelTemperature\"])\n",
        "\n",
        "        # Using langchain function create_react_agent\n",
        "        self.agent = create_react_agent(\n",
        "            llm,\n",
        "            self.tools,\n",
        "            prompt\n",
        "        )\n",
        "\n",
        "        # Using langchain class AgentExecutor\n",
        "        self.reactAgentExecutor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            verbose= self.config.get(\"reactVerbosity\", False)\n",
        "        )\n",
        "\n",
        "    def run(self, question: str) -> str:\n",
        "        if not self.reactAgentExecutor:\n",
        "            raise ValueError(\"React agent not initialized. Call initialize_agent() first.\")\n",
        "        return self.reactAgentExecutor.invoke({\"input\": question})['output']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZqH_ncryqf9R",
      "metadata": {
        "id": "ZqH_ncryqf9R"
      },
      "source": [
        "# RAG Chain class\n",
        "<font color=\"#d3d3d3\">`RAGChain` (Custom-defined class):\n",
        " A custom class implementing a Retrieval-Augmented Generation (RAG) system that combines document retrieval with language model generation by retrieving the most relevant documents and using them to generate an answer.\n",
        " - `reactObj`: An optional instance of the React class, used to interact with external tools (like web search or company directors information).\n",
        "\n",
        "<font color=\"#d3d3d3\">`__init __`:\n",
        "Initializes the RAG chain with configuration and a vector store retriever\n",
        "\n",
        "- <font color=\"#d3d3d3\">Stores a reference to the `baseRetriever` (from the VectorStore class)\n",
        "- `ChatOpenAI` (Langchain class):\n",
        " LangChain class for interfacing with OpenAI's chat models\n",
        "- prompt is an instance of `PromptTemplate` (a LangChain class) to build the RAG prompt.\n",
        "- Sets the number of documents to be selected from the vector store and when using query decomposition.\n",
        "- `retrieverWithReranking`: It makes the reranking retriever available for use within the RAGChain instance.\n",
        "- If a `reactObj` is provided, it's initialized using `initializeAgent`.\n",
        "\n",
        "\n",
        "<font color=\"#d3d3d3\">Q) Why did we use Langchain's `PromptTemplate` class instead of a string template directly?\n",
        "- <font color=\"#d3d3d3\">It ensures dynamic input substitution with validation, reducing the risk of formatting errors or missing placeholders.\n",
        "- It integrates seamlessly with LangChain's modular components, enhancing composability in workflows and simplifying maintenance.\n",
        "\n",
        "`query`:\n",
        "The main interface method that processes a question through the entire RAG pipeline and returns the LLM-generated answer.\n",
        "- If `reactObj` is available, it calls self.reactObj.run to leverage external tools.\n",
        "- Otherwise, it builds the chain using `_makeChain` and invokes it with the question to get the answer.\n",
        "- Returns the final answer.\n",
        "\n",
        "<font color=\"#d3d3d3\">`_subQueryRetrieval`:\n",
        "- <font color=\"#d3d3d3\">Only used when a query decomposer is provided\n",
        "- Decomposes the main question into sub-queries using the query decomposer\n",
        "- For each sub-query:\n",
        "  * <font color=\"#d3d3d3\">Retrieves relevant documents using the reranking retriever\n",
        "  * Formats the retrieved documents\n",
        "  * Concatenates the formatted documents with line breaks\n",
        "- Returns the combined context string\n",
        "\n",
        "Note: Reranker is not optional in this scenario, so there is no fReranker option in this code.\n",
        "\n",
        "`_retrieveContext`:\n",
        "- If a query decomposer exists, calls `_subQueryRetrieval`\n",
        "- else, retrieves documents using the base retriever and formats them\n",
        "- Uses `_formatDocs` to format (reranked) retrieved chunks into a string that can be used as context for the language model. <font color=\"#d3d3d3\">`_formatDocs` does the following:\n",
        "  * <font color=\"#d3d3d3\">Selects only the first `self.numSelectedDocuments` chunks retrieved from vector store.\n",
        "  *  `_getPrefix(doc)` extracts the company name added as metadata to each chunk in `loadAndProcessFilings`, and concatenates it to the text content of each chunk (`doc.page_content`).\n",
        "  * Joins all the chunks into a single string.  It adds 2 line breaks `\\n\\n` as a seperator between two chunks.\n",
        "\n",
        "<font color=\"#d3d3d3\">`_makeChain`:\n",
        "Constructs the processing pipeline using LangChain's Runnable interface components.\n",
        "\n",
        "- <font color=\"#d3d3d3\">`RunnableLambda` is a LangChain utility that wraps a callable Python function (in this case, _retrieveContext) and makes it compatible with LangChain pipelines. The output of this step is the retrieved chunks combined into a string generated by `retrieveContext`.\n",
        "- `RunnablePassthrough` is a LangChain utility that simply passes the user's question to the next step in the pipeline without modifying it. This is useful when you need to include the question alongside the context in a prompt for generating response.\n",
        "- Next step in pipeline replaces placeholder variables {context} and {question} in prompt instance with the retrieved context from `RunnableLambda` and the original question from `RunnablePassthrough`.\n",
        "- `ChatOpenAI` is a LangChain interface for interacting with OpenAI's chat models. This component invokes the OpenAI language model (e.g., GPT-4) to generate a response based on the prompt.\n",
        "- `StrOutputParser` is a LangChain utility that parses the raw output of the LLM (such as the model used via ChatOpenAI) into a clean string format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-E5-w5iF2Y6X",
      "metadata": {
        "id": "-E5-w5iF2Y6X"
      },
      "outputs": [],
      "source": [
        "class RAGChain:\n",
        "    def __init__(self, config: Dict[str, Any], baseRetriever: VectorStore, queryDecomposerObj=None, reactObj=None):\n",
        "        self.config = config\n",
        "        self.baseRetriever = baseRetriever\n",
        "        self.llm = ChatOpenAI(model=config[\"ragAnswerModel\"], temperature=config[\"ragAnswerModelTemeprature\"])\n",
        "        self.prompt = PromptTemplate.from_template(config[\"ragPromptTemplate\"])\n",
        "        self.numSelectedDocuments = config[\"numSelectedDocuments\"]\n",
        "        self.numQueryDecomposerSelectedDocuments = config[\"numQueryDecomposerSelectedDocuments\"]\n",
        "        self.queryDecomposerObj = queryDecomposerObj\n",
        "        self.reactObj = reactObj\n",
        "        self.retrieverWithReranking = createReranker(self.config, self.baseRetriever)\n",
        "        if reactObj:\n",
        "            self.reactObj.initializeAgent(self.retrieverWithReranking)\n",
        "\n",
        "    def _subQueryRetrieval(self, question: str) -> str:\n",
        "        subqueries = self.queryDecomposerObj.decomposeQuery(question)\n",
        "        contextString = ''\n",
        "\n",
        "        for subq in subqueries:\n",
        "            docs = self.retrieverWithReranking.invoke(subq)\n",
        "            contextString += self._formatDocs(docs)\n",
        "            contextString += \"\\n\\n\"\n",
        "        return contextString\n",
        "\n",
        "    def _retrieveContext(self, question: str) -> str:\n",
        "        if self.queryDecomposerObj is not None:\n",
        "            return self._subQueryRetrieval(question)\n",
        "        else:\n",
        "            retrievedDocs = self.baseRetriever.invoke(question)\n",
        "            return self._formatDocs(retrievedDocs)\n",
        "\n",
        "    def _formatDocs(self, docs: List[Document]) -> str:\n",
        "        def _getPrefix(doc):\n",
        "            company = doc.__dict__.get('metadata', {}).get('company', '')\n",
        "            return company\n",
        "\n",
        "        return \"\\n\\n\".join(f\"{_getPrefix(doc)}\\n{doc.page_content}\" for doc in docs[:self.numSelectedDocuments])\n",
        "\n",
        "    def _makeChain(self):\n",
        "        return (\n",
        "            {\n",
        "                \"context\": RunnableLambda(self._retrieveContext),\n",
        "                \"question\": RunnablePassthrough()\n",
        "            }\n",
        "            | self.prompt\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        if self.reactObj:\n",
        "            return self.reactObj.run(question)\n",
        "        else:\n",
        "            return self._makeChain().invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BgWBZlOwV38C",
      "metadata": {
        "id": "BgWBZlOwV38C"
      },
      "source": [
        "# PreProcessing Data\n",
        "\n",
        "<font color=\"#d3d3d3\">So far, we have defined the classes (blueprint) needed for implementing a simple RAG prototype on 10-K filings. These classes provide the foundational structure for tasks like data retrieval and processing. Next, we will create instances to operationalize these tasks and demonstrate preprocessing functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i7vrk_fW3Tox",
      "metadata": {
        "id": "i7vrk_fW3Tox"
      },
      "outputs": [],
      "source": [
        "config = defaultConfig.copy() # Creates a separate copy of the default configuration dictionary (defaultConfig) so that any subsequent changes won't alter the original default settings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mgIBjW7LVcb6",
      "metadata": {
        "id": "mgIBjW7LVcb6"
      },
      "source": [
        "## Chunking\n",
        "* <font color=\"#d3d3d3\">Creates a `FilingsPreProcessor` instance, responsible for downloading webpages and splitting filings into smaller, more manageable chunks.\n",
        "* Uses the `loadAndProcessFilings` to load, split, and process the filing URLs listed in config[\"companyFilingUrls\"].\n",
        "* Returns chunks ready for indexing in Vector DB.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mWWhcqSR4Wly",
      "metadata": {
        "id": "mWWhcqSR4Wly"
      },
      "outputs": [],
      "source": [
        "preProcessorObj = FilingsPreProcessor(config)\n",
        "processedFilings, directorStrings = preProcessorObj.loadAndProcessFilings(config[\"companyFilingUrls\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NrBQlxCS8yX",
      "metadata": {
        "id": "9NrBQlxCS8yX"
      },
      "source": [
        "## Embedding and Vector Storage\n",
        "-<font color=\"#d3d3d3\"> Instantiates an `EmbeddingService` object, which provides text embedding functionality (via the Hugging Face Inference API in this example).\n",
        "- <font color=\"#d3d3d3\">Instantiates a `VectorStore` object\n",
        "- Builds a VectorStore object that internally uses a FAISS store to index and retrieve documents based on vector similarity.\n",
        "- Uses embedding object created earlier to generate embeddings of chunks added by `addDocuments`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h2Hq0bzP32tb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Hq0bzP32tb",
        "outputId": "abc6b011-8f15-4b65-f30d-d0be8f0fc9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully embedded 2027/2027 texts\n"
          ]
        }
      ],
      "source": [
        "embeddingServiceObj = EmbeddingService(config)\n",
        "vectorStoreObj = VectorStore(config, embeddingServiceObj)\n",
        "vectorStoreObj.addDocuments(processedFilings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4HpSP_CMui2",
      "metadata": {
        "id": "d4HpSP_CMui2"
      },
      "outputs": [],
      "source": [
        "queryDecomposerObj = QueryDecomposer(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oHXzA7fi24yI",
      "metadata": {
        "id": "oHXzA7fi24yI"
      },
      "outputs": [],
      "source": [
        "reactObj = React(config, directorStrings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ej7sUEUkQ_I",
      "metadata": {
        "id": "0ej7sUEUkQ_I"
      },
      "source": [
        "# User Query Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cf1Ir62dnY2o",
      "metadata": {
        "id": "Cf1Ir62dnY2o"
      },
      "source": [
        "## RAG Chain pipeline\n",
        "<font color=\"#d3d3d3\">This pipeline assembles all the necessary pieces:\n",
        "- <font color=\"#d3d3d3\">Takes the configuration settings (`config`).</font>\n",
        "\n",
        "- <font color=\"#d3d3d3\">Connects the RAG pipeline to the vector store through the retriever (`vectorStoreObj.retriever`) and fetches the relevant chunks from vector store based on the user query passed to `ragObj.query` and reranks them.\n",
        "\n",
        "- <font color=\"#d3d3d3\">Packages the entire pipeline into a single object (`ragObj`) that represents your complete RAG system. `ragObj` is an instance of the `RAGChain`class. You can think of it like a container or a handle that is used to interact with the entire RAG system.\n",
        "\n",
        "- <font color=\"#d3d3d3\">Since the `queryDecomposerObj` is not provided, its default value of `None` will be used in the `RAGChain`. This means the retrieved chunks will not be processed through the Query Decomposer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xGEyiCSn39F1",
      "metadata": {
        "id": "xGEyiCSn39F1"
      },
      "outputs": [],
      "source": [
        "# rerankerRagObj = RAGChain(config, vectorStoreObj.retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89MzbKN1JEMm",
      "metadata": {
        "id": "89MzbKN1JEMm"
      },
      "source": [
        "<font color=\"#d3d3d3\">The only change from the last statement is that `queryDecomposerObj`, which is `QueryDecomposer` instance, is passed to `RAGChain`. This means the retrieved chunks will be processed through Query Decomposer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EUx_jHNoDQkW",
      "metadata": {
        "id": "EUx_jHNoDQkW"
      },
      "outputs": [],
      "source": [
        "# queryDecomposerRagObj = RAGChain(config, vectorStoreObj.retriever, queryDecomposerObj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UBkpGFRo3UvX",
      "metadata": {
        "id": "UBkpGFRo3UvX"
      },
      "outputs": [],
      "source": [
        "reactRagObj = RAGChain(config, vectorStoreObj.retriever, queryDecomposerObj, reactObj)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VKRGIHSgn0_U",
      "metadata": {
        "id": "VKRGIHSgn0_U"
      },
      "source": [
        "## Pass User Query\n",
        "This is where you pass the user's query to RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sUghzVd-3QHm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUghzVd-3QHm",
        "outputId": "e9d6f5dd-b461-4685-d3fb-fd9976a71819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer the question, I need to gather information on the directors of Tesla, their LinkedIn handles, Tesla's financial goals for this year, and the next auto show Tesla will participate in.\n",
            "\n",
            "First, I'll start by finding the directors of Tesla and their LinkedIn handles.\n",
            "\n",
            "Action: Company Directors Information\n",
            "\n",
            "Action Input: Tesla, true\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mDirectors of Tesla: Elon Musk (LinkedIn: https://www.linkedin.com/in/elon-musk-a4647575?trk=news_storyline_guest-text); Robyn Denholm (LinkedIn: https://au.linkedin.com/in/robyn-denholm-a807795); Ira Ehrenpreis (LinkedIn: https://www.linkedin.com/in/iraehrenpreis); Joseph Gebbia (LinkedIn: https://www.linkedin.com/in/john-coppola-jr-4b2ab395); James Murdoch (LinkedIn: https://www.linkedin.com/in/jamesrmurdoch); Kimbal Musk (LinkedIn: https://www.linkedin.com/in/kimbal); JB Straubel (LinkedIn: https://www.linkedin.com/in/jb-straubel-b694981); Kathleen Wilson-Thompson (LinkedIn: https://www.linkedin.com/in/kathleen-wilson-thompson-275654201)\u001b[0m\u001b[32;1m\u001b[1;3mI have obtained the list of Tesla's directors and their LinkedIn handles. Next, I need to find information about Tesla's financial goals for this year.\n",
            "\n",
            "Action: Vector Reranker Search\n",
            "\n",
            "Action Input: \"Tesla financial goals 2023\"\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mtsla-20231231false00013186052023FYhttp://fasb.org/us-gaap/2023#AccountingStandardsUpdate202006Memberhttp://fasb.org/us-gaap/2023#RevenueFromContractWithCustomerExcludingAssessedTaxhttp://fasb.org/us-gaap/2023#RevenueFromContractWithCustomerExcludingAssessedTaxhttp://fasb.org/us-gaap/2023#RevenueFromContractWithCustomerExcludingAssessedTaxP7YP3Y0.048414http://www.tesla.com/20231231#AccruedAndOtherCurrentLiabilitieshttp://www.tesla.com/20231231#AccruedAndOtherCurrentLiabilitieshttp://fasb.org/us-\n",
            "\n",
            "http://www.tesla.com/20231231#LeasedAssetsNethttp://fasb.org/us-gaap/2023#PropertyPlantAndEquipmentNet\n",
            "\n",
            "required to spend RMB 14.08 billion in capital expenditures by the end of 2023, which has been achieved in 2023, and to generate RMB 2.23 billion of annual tax revenues starting at the end of 2023. As of December 31, 2023, we have met and expect to meet the tax revenue requirements based on our current level of spend and sales.Legal ProceedingsLitigation Relating to 2018 CEO Performance AwardOn June 4, 2018, a purported Tesla stockholder filed a putative class and derivative action in the\u001b[0m\u001b[32;1m\u001b[1;3mI have found some information regarding Tesla's financial goals for 2023, including capital expenditures and tax revenue targets. However, I need more specific details about their financial goals for this year.\n",
            "\n",
            "Action: WebSearch\n",
            "\n",
            "Action Input: \"Tesla financial goals 2023\"\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mTitle: tsla-20231231-gen.pdf\n",
            "Snippet: We also have audited the Company's internal control over financial reporting as of December 31, 2023, based on criteria established in.\n",
            "Link: https://ir.tesla.com/_flysystem/s3/sec/000162828024002390/tsla-20231231-gen.pdf\n",
            "\n",
            "Title: There are plausible explanations for the $1.4 billion gap on ...\n",
            "Snippet: Tesla is making big bets on AI, attracting attention to its financial statements. The Financial Times first noted a $1.4 billion discrepancy ...\n",
            "Link: https://finance.yahoo.com/news/tesla-1-4-billion-seems-111300909.html\n",
            "\n",
            "Title: Tesla Investor Relations\n",
            "Snippet: Tesla's mission is to accelerate the world's transition to sustainable energy. Today, Tesla builds not only all-electric vehicles but also infinitely ...\n",
            "Link: https://ir.tesla.com/\u001b[0m\u001b[32;1m\u001b[1;3mI have gathered some information about Tesla's financial goals for 2023, including capital expenditures and tax revenue targets. However, I need more specific details about their financial goals for this year. I will now look for information on the next auto show Tesla will participate in.\n",
            "\n",
            "Action: WebSearch\n",
            "\n",
            "Action Input: \"Tesla next auto show 2023\"\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mTitle: New York International Auto Show – The oldest and largest ...\n",
            "Snippet: The New York Auto Show returns to the Big Apple from April 18-27, 2025, for an unforgettable celebration of our 125th anniversary, and you're invited to be part ...\n",
            "Link: https://www.autoshowny.com/\n",
            "\n",
            "Title: Events\n",
            "Snippet: Can't find an event you're looking for? Receive notifications for future events. Experience New Model Y at Tesla Fremont. Feb 17 – Mar 31, 2025.\n",
            "Link: https://www.tesla.com/events\n",
            "\n",
            "Title: Tesla booted from Vancouver International Auto Show over ...\n",
            "Snippet: Worries about safety prompted the Vancouver International Auto Show to exclude Elon Musk's Tesla from the yearly exhibition.\n",
            "Link: https://www.foxbusiness.com/technology/tesla-booted-from-vancouver-international-auto-show-over-safety-attendees\u001b[0m\u001b[32;1m\u001b[1;3mI need to refine my search to find the specific auto show Tesla will participate in next.\n",
            "\n",
            "Action: WebSearch\n",
            "\n",
            "Action Input: \"Tesla upcoming auto show participation 2023\"\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mTitle: Tesla to attend 2023 Detroit Auto Show\n",
            "Snippet: Tesla would be a far different company when it attends the show this year. The 2023 Detroit Auto Show would run from September 13 to 24 at the Huntington Place ...\n",
            "Link: https://www.teslarati.com/tesla-to-attend-2023-detroit-auto-show/\n",
            "\n",
            "Title: Tesla will participate in 2023 Munich auto show\n",
            "Snippet: Tesla's participation is a big plus for the show's organizers as the attendance of some well-known automaking groups is less certain.\n",
            "Link: https://www.autonews.com/munich-auto-show/tesla-will-participate-2023-munich-auto-show/\n",
            "\n",
            "Title: Tesla booted from Vancouver International Auto Show over ...\n",
            "Snippet: Worries about safety prompted the Vancouver International Auto Show to exclude Elon Musk's Tesla from the yearly exhibition.\n",
            "Link: https://www.foxbusiness.com/technology/tesla-booted-from-vancouver-international-auto-show-over-safety-attendees\u001b[0m\u001b[32;1m\u001b[1;3mI have gathered the necessary information to answer the question.\n",
            "\n",
            "Final Answer: The directors of Tesla and their LinkedIn handles are as follows:\n",
            "- Elon Musk (LinkedIn: [Elon Musk](https://www.linkedin.com/in/elon-musk-a4647575?trk=news_storyline_guest-text))\n",
            "- Robyn Denholm (LinkedIn: [Robyn Denholm](https://au.linkedin.com/in/robyn-denholm-a807795))\n",
            "- Ira Ehrenpreis (LinkedIn: [Ira Ehrenpreis](https://www.linkedin.com/in/iraehrenpreis))\n",
            "- Joseph Gebbia (LinkedIn: [Joseph Gebbia](https://www.linkedin.com/in/john-coppola-jr-4b2ab395))\n",
            "- James Murdoch (LinkedIn: [James Murdoch](https://www.linkedin.com/in/jamesrmurdoch))\n",
            "- Kimbal Musk (LinkedIn: [Kimbal Musk](https://www.linkedin.com/in/kimbal))\n",
            "- JB Straubel (LinkedIn: [JB Straubel](https://www.linkedin.com/in/jb-straubel-b694981))\n",
            "- Kathleen Wilson-Thompson (LinkedIn: [Kathleen Wilson-Thompson](https://www.linkedin.com/in/kathleen-wilson-thompson-275654201))\n",
            "\n",
            "Tesla's financial goals for 2023 include significant capital expenditures and generating annual tax revenues. They are required to spend RMB 14.08 billion in capital expenditures by the end of 2023 and generate RMB 2.23 billion of annual tax revenues starting at the end of 2023.\n",
            "\n",
            "The next auto show Tesla will participate in is the 2023 Munich Auto Show.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The directors of Tesla and their LinkedIn handles are as follows:\n",
            "- Elon Musk (LinkedIn: [Elon Musk](https://www.linkedin.com/in/elon-musk-a4647575?trk=news_storyline_guest-text))\n",
            "- Robyn Denholm (LinkedIn: [Robyn Denholm](https://au.linkedin.com/in/robyn-denholm-a807795))\n",
            "- Ira Ehrenpreis (LinkedIn: [Ira Ehrenpreis](https://www.linkedin.com/in/iraehrenpreis))\n",
            "- Joseph Gebbia (LinkedIn: [Joseph Gebbia](https://www.linkedin.com/in/john-coppola-jr-4b2ab395))\n",
            "- James Murdoch (LinkedIn: [James Murdoch](https://www.linkedin.com/in/jamesrmurdoch))\n",
            "- Kimbal Musk (LinkedIn: [Kimbal Musk](https://www.linkedin.com/in/kimbal))\n",
            "- JB Straubel (LinkedIn: [JB Straubel](https://www.linkedin.com/in/jb-straubel-b694981))\n",
            "- Kathleen Wilson-Thompson (LinkedIn: [Kathleen Wilson-Thompson](https://www.linkedin.com/in/kathleen-wilson-thompson-275654201))\n",
            "\n",
            "Tesla's financial goals for 2023 include significant capital expenditures and generating annual tax revenues. They are required to spend RMB 14.08 billion in capital expenditures by the end of 2023 and generate RMB 2.23 billion of annual tax revenues starting at the end of 2023.\n",
            "\n",
            "The next auto show Tesla will participate in is the 2023 Munich Auto Show.\n"
          ]
        }
      ],
      "source": [
        "print(reactRagObj.query(\"Who are the directors of Tesla. What are their linkedin handles? What are the financial goals of tesla this year. What is the next auto show that Tesla will participate in.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ja1y5C6UACq-",
      "metadata": {
        "id": "ja1y5C6UACq-"
      },
      "outputs": [],
      "source": [
        "#print(rerankerRagObj.query(\"Who are the directors of Tesla. What are their linkedin handles? What are the financial goals of tesla this year. What is the next auto show that Tesla will participate in.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L6aFlJ9n8_sH",
      "metadata": {
        "id": "L6aFlJ9n8_sH"
      },
      "outputs": [],
      "source": [
        "#print(queryDecomposerRagObj.query(\"Who are the directors of Tesla. What are their linkedin handles? What are the financial goals of tesla this year. What is the next auto show that Tesla will participate in.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RwWXzh1fLZ57",
      "metadata": {
        "id": "RwWXzh1fLZ57"
      },
      "source": [
        "## Final Response Analysis\n",
        "### How to Trace Agentic steps using Langsmith\n",
        "\n",
        "#### AgentExecutor:\n",
        "- Input: user's query as input\n",
        "- Output: final output of the agent after going through series of steps\n",
        "\n",
        "#### Process to get Company director information\n",
        "##### 1st ChatOpenAI immediately after AgentExecutor:\n",
        "- Input: ReAct's prompt populated with {tools} and user's {question}\n",
        "- Output: ChatGPT api returns the thought, action and action input, which are to call Company director information tool and get the information for company Tesla, and set the request to get linkedin handles as true\n",
        "\n",
        "##### Company Directors information\n",
        "- Input: Tesla, true\n",
        "- Output: Directors information including their linkedin handle\n",
        "- Process:\n",
        "  * Retrieves stored last portion of 10k statement for Tesla\n",
        "ChatOpenAI (You may need to expand Company Directors Information)\n",
        "  * Input: populates retrieved last portion of 10k in the prompt template for extracting directors names\n",
        "  * Output: list of director names in csv format\n",
        "  * Websearch Tool: Figure out how you can add tracing for the call of WebSearch tool\n",
        "\n",
        "\n",
        "#### Process to get Tesla's financial goals\n",
        "##### ChatOpenAI after collapsing Company Directors Information\n",
        "- Input: Earlier React's prompt + observation: company director information\n",
        "- Output: Decides it needs to use vector database search\n",
        "\n",
        "##### Vector Reranker Search (collapse it)\n",
        "- Input: What are Tesla's financial goals for this year?\n",
        "- Output: Reranked top 3 chunks\n",
        "On expanding it you will see\n",
        "ContextualCompressionRetriever (on expanding)\n",
        "- VectorStoreRetriever: Gets the above question and returns top 12 chunks from FAISS; Next, reranker picks top 5\n",
        "\n",
        "\n",
        "#### Process to get Next auto show information\n",
        "##### ChatOpenAI after collapsing Vector reranker Search\n",
        "- Input: Last React's prompt + observation: top 3 reranked chunks for Tesla's financial goals\n",
        "- Output: Decides it news to use Websearch tool to get information about Next auto show participation\n",
        "##### WebSearch tool\n",
        "- Input: Next auto show Tesla will participate in 2023\n",
        "- Output: Retrieves top 3 search results for the search term\n",
        "\n",
        "\n",
        "#### Final prompt to generate response\n",
        "Last ChatOpenAI\n",
        "- Input: Last React's prompt + observation: top 3 results from search engine\n",
        "- Observation part of the prompt:\n",
        "\n",
        "> First, I will retrieve the names of the company directors for Tesla and their LinkedIn handles.\n",
        "- Action: Company Directors Information\n",
        "- Action Input: Tesla, true\n",
        "\n",
        "> Directors of Tesla: Robyn Denholm (LinkedIn: https://au.linkedin.com/in/robyn-denholm-a807795); Ira Ehrenpreis (LinkedIn: https://www.linkedin.com/in/iraehrenpreis); Joseph Gebbia (LinkedIn: https://www.linkedin.com/in/john-coppola-jr-4b2ab395); James Murdoch (LinkedIn: https://www.linkedin.com/in/jamesrmurdoch); Kimbal Musk (LinkedIn: https://www.linkedin.com/in/kimbal); JB Straubel (LinkedIn: https://www.linkedin.com/in/jb-straubel-b694981); Kathleen Wilson-Thompson (LinkedIn: https://www.linkedin.com/in/kathleen-wilson-thompson-275654201)I have obtained the names and LinkedIn handles of the directors of Tesla. Next, I will gather information on Tesla's financial goals for this year.\n",
        "- Action: Vector Reranker Search\n",
        "- Action Input: What are Tesla's financial goals for this year?\n",
        "\n",
        ">  required to spend RMB 14.08 billion in capital expenditures by the end of 2023, which has been achieved in 2023, and to generate RMB 2.23 billion of annual tax revenues starting at the end of 2023. As of December 31, 2023, we have met and expect to meet the tax revenue requirements based on our current level of spend and sales.Legal ProceedingsLitigation Relating to 2018 CEO Performance AwardOn June 4, 2018, a purported Tesla stockholder filed a putative class and derivative action in the\n",
        "\n",
        ">  platform, as well as our ability to add to our available sources of battery cell supply by manufacturing our own cells that we are developing to have high-volume output, lower capital and production costs and longer range. Our goals are to improve vehicle performance, decrease production costs and increase affordability and customer awareness. These plans are subject to uncertainties inherent in establishing and ramping manufacturing operations, which may be exacerbated by new product and\n",
        "\n",
        ">  financial and other services related to our products. Additionally, we are increasingly focused on products and services based on artificial intelligence, robotics and automation.In 2023, we produced 1,845,985 consumer vehicles and delivered 1,808,581 consumer vehicles. We are currently focused on increasing vehicle production, capacity and delivery capabilities, reducing costs, improving and developing our vehicles and battery technologies, vertically integrating and localizing our supplyI have gathered information on Tesla's financial goals for this year, which include increasing vehicle production, capacity, and delivery capabilities, reducing costs, improving and developing vehicles and battery technologies, and focusing on artificial intelligence, robotics, and automation. Now, I will search for information on the next auto show that Tesla will participate in.\n",
        "\n",
        ">  - Action: WebSearch\n",
        ">  - Action Input: Next auto show Tesla will participate in 2023\n",
        "\n",
        ">  Title: Tesla to Join 2023 Detroit Auto Show\n",
        "  Snippet: Tesla will join the 2023 Detroit Auto Show that will take place September 13-24. The company will showcase its vehicles at the Powering Michigan EV Experience.\n",
        "  Link: https://driveteslacanada.ca/news/tesla-to-join-2023-detroit-auto-show/\n",
        "\n",
        ">  Title: Tesla Attending 2023 Shenzhen Auto Show, Might ...\n",
        "  Snippet: Chinese media confirms that Tesla will attend this year's Shenzhen Auto Show on June 16, its first Chinese auto show presence since April 2021.\n",
        "  Link: https://www.autoevolution.com/news/tesla-is-attending-the-2023-shenzhen-auto-show-might-showcase-the-refreshed-model-3-216533.html\n",
        "\n",
        ">  Title: 2023 Detroit Auto Show: Everything You Need to Know\n",
        "  Snippet: This year's show, taking place Sept. 13-24 at Huntington Place, will continue to expand its focus from regular vehicles and will include EVs, the future ...\n",
        "  Link: https://michauto.org/2023-detroit-auto-show-everything-you-need-to-know/I have gathered information on the next auto shows Tesla will participate in. Tesla will join the 2023 Detroit Auto Show, taking place from September 13-24, and it also attended the 2023 Shenzhen Auto Show on June 16.\n",
        "\n",
        "> I now have all the necessary information to answer the question.\n",
        "\n",
        "  >Final Answer: The directors of Tesla are Robyn Denholm, Ira Ehrenpreis, Joseph Gebbia, James Murdoch, Kimbal Musk, JB Straubel, and Kathleen Wilson-Thompson. Their LinkedIn handles are as follows:\n",
        "  - Robyn Denholm: [LinkedIn](https://au.linkedin.com/in/robyn-denholm-a807795)\n",
        "  - Ira Ehrenpreis: [LinkedIn](https://www.linkedin.com/in/iraehrenpreis)\n",
        "  - Joseph Gebbia: [LinkedIn](https://www.linkedin.com/in/john-coppola-jr-4b2ab395)\n",
        "  - James Murdoch: [LinkedIn](https://www.linkedin.com/in/jamesrmurdoch)\n",
        "  - Kimbal Musk: [LinkedIn](https://www.linkedin.com/in/kimbal)\n",
        "  - JB Straubel: [LinkedIn](https://www.linkedin.com/in/jb-straubel-b694981)\n",
        "  - Kathleen Wilson-Thompson: [LinkedIn](https://www.linkedin.com/in/kathleen-wilson-thompson-275654201)\n",
        "\n",
        "  Tesla's financial goals for this year include increasing vehicle production, capacity, and delivery capabilities, reducing costs, improving and developing vehicles and battery technologies, and focusing on artificial intelligence, robotics, and automation.\n",
        "\n",
        "  The next auto show Tesla will participate in is the 2023 Detroit Auto Show, taking place from September 13-24. Tesla also attended the 2023 Shenzhen Auto Show on June 16."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TgcZSwXy8o9q",
      "metadata": {
        "id": "TgcZSwXy8o9q"
      },
      "source": [
        "## Trace in LangSmith\n",
        "Open https://LangSmith.com\n",
        "\n",
        "* Open project: 10_K_React\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}