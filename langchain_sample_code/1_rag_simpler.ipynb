{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to Run this Notebook?\n",
        "1. Generate API key for OpenAI (ChatGPT): https://platform.openai.com/settings/organization/api-keys\n",
        "Make sure to save the API key. You'll get to see the key only once at the time of generation. If you miss copying the key, you may need to generate a new key.\n",
        "2. Generate API token for HuggingFace Source: https://huggingface.co/settings/tokens/new?tokenType=write\n",
        "Just like last time, make sure to save this API key as well.\n",
        "3. Click on ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAAkCAIAAABXBb6UAAADK0lEQVRYCe2Xz0vrQBDH/VezS2hUgjk09FJFEX+AB8VICMaTBS+FQgXRg4egYjw09CRY8SRBkcbQYggtsbsPXBj2JbE0zYvvPegcwnR2M/PpN7MTMkf/K5v7r2jpDLfI5zVTd6YuKDB9MwRBYNu2aZr6l5mmadt2EASQughnGtwgCA4PDzHGQsIwxoZh9Hq9IlgpzT7I2u22LMsJzt8Csiy32+0iiLOpe3d3J0kSoGGMK5XKwcGBruvValUURVgSRfH6+vqPE2fAdV13aWmJASGEDMPo9/s8UBiGJycn0CSLi4uPj4/8hvz+pLifn5+7u7uMVRTF09NTQkiyPCHEsiyQeX5+XsluZ2dnycwsMilup9OBNjg6OkplZRkJIbVajf2x6a71ej0vbr1eZ7UVRXl5eWHp3t/fd3Z20Jetrq52u10W931fVdXpWAVByItLCNnb22PlNU1j0g4Gg+3tbZ5pfX0d5u7FxYWexTRNg4GTFzcMw7W1NUYGuZ6enhYWFnhcjLHjOEzgrNfUEskkE/Vuaq77+/tSqcTjCoJg23ayxiSR1BLJGyfCHQ6HW1tbjOz4+JhlSTaoLMvPz89s9ePjw/vewjBk26Io8n3f8zzXdVdWVmIPcEpcSqlpmizX8vIyNOjNzQ0ILIri5eUlKzAYDDY2NmLC8z+ho/iBAxtgdXrc29tbhJAgCBhjy7Igked5V1dXlmW9vb1B0HEcGL0AwTsAVBRuv9+vVqusZLlcdl0X4GLO6+truVzm4ZJ+4biUUsuy4AWrKMrDw0MMlFLqum6lUmF8CKHz83No4NTuBHUlSWq1WmwzdHYy/0RHjd0WRZGu6yAVxnhzc9O27W6363me4zj7+/t8D+i6HkURlOTPfq1WY2StVou9LCVJ6nQ6sPk7JwMupTSKIsMwWBMDd9JBCGmaFhOJx03eUggupXQ0GjWbTRgIycKlUqnZbI5Go5hCfweXQQRB0Gg0VFUFpRFCqqo2Gg0Yc/8QLo8ShuFwOOQjqT4hpNfrwcmLOb7v842emmGaj5/vEv1MPNtR+xmmMVVmuGPEyb00Uze3hGMSzNQdI07upV9MOsjO+LCzRgAAAABJRU5ErkJggg==) icon in the left menu bar of this Notebook\n",
        "4. Click `+ Add new secret `\n",
        "- Add OpenAI key:\n",
        "  * Under Name copy paste: `OPENAI_KEY`\n",
        "  * Under Value copy paste: OpenAI key you saved earlier\n",
        "\n",
        "- Add HuggingFace key:\n",
        "  * Under Name copy paste: `HF_KEY`\n",
        "  * Under Value copy paste: HF key you saved earlier\n",
        "\n",
        "5. Enable access to the keys for this notebook by toggling the radio buttons in the `Secrets` section.\n",
        "6. Close the `Secrets` section once done.\n",
        "7. Click `Run all` under the `Runtime` in top menu to execute this notebook.\n"
      ],
      "metadata": {
        "id": "BeskM2NPo-AF"
      },
      "id": "BeskM2NPo-AF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Setup\n",
        "**Install Frameworks**\n",
        "\n",
        "`langchain`, `langchain_core`, and `langchain_community` are components of the LangChain framework.\n",
        "\n",
        "`faiss-cpu` is Facebook AI's similarity search library, optimized for CPU usage.\n",
        "\n",
        "`openai==1.56.2` and `langchain_openai` are the OpenAI Python client library and LangChain's OpenAI integration package respectively."
      ],
      "metadata": {
        "id": "a-jK0idCyWtt"
      },
      "id": "a-jK0idCyWtt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8f8207",
      "metadata": {
        "id": "5e8f8207"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_core langchain_community faiss-cpu openai==1.56.2 langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**\n",
        "\n"
      ],
      "metadata": {
        "id": "uq9Q1b21Ufk-"
      },
      "id": "uq9Q1b21Ufk-"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import requests\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "k1XadP0546aj"
      },
      "id": "k1XadP0546aj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "082f85f0",
      "metadata": {
        "id": "082f85f0"
      },
      "source": [
        "**Project's Secret Keys Setup**\n",
        "\n",
        "This code sets up authentication credentials by retrieving API keys from Colab's `Secrets` storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da18250",
      "metadata": {
        "id": "1da18250"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_KEY')\n",
        "hfToken = userdata.get('HF_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration Dictionary\n",
        "\n",
        "Contains settings for document processing and RAG system, including parameters for text chunking, embedding generation, document retrieval, model specifications, prompts used and API endpoints."
      ],
      "metadata": {
        "id": "2k78A3s4ExZq"
      },
      "id": "2k78A3s4ExZq"
    },
    {
      "cell_type": "code",
      "source": [
        "defaultConfig = {\n",
        "    # Document processing settings\n",
        "    \"chunkSize\": 500,\n",
        "    \"chunkOverlap\": 50,\n",
        "    \"userAgentHeader\": \"YourCompany-ResearchBot/1.0 (your@email.com)\",\n",
        "\n",
        "    # Embedding settings\n",
        "    \"embeddingApiUrl\": \"https://api-inference.huggingface.co/models/BAAI/bge-base-en-v1.5\",\n",
        "    \"embeddingDim\": 768,\n",
        "\n",
        "    #Defalt document\n",
        "    \"defaultDocument\" : Document(\n",
        "        page_content = \"This is a default document.\",\n",
        "        metadata = {\"source\": \"default\"}\n",
        "    ),\n",
        "\n",
        "    # Vector store settings\n",
        "    \"numRetrievedDocuments\": 12,\n",
        "\n",
        "    # Document formater settings\n",
        "    \"numSelectedDocuments\": 5,\n",
        "\n",
        "    # Model settings\n",
        "    \"ragAnswerModel\": \"gpt-4o\",\n",
        "    \"ragAnswerModelTemperature\": 0.7,\n",
        "\n",
        "    # URLs to process\n",
        "    \"companyFilingUrls\": [\n",
        "        (\"Tesla\", \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\")\n",
        "    ],\n",
        "\n",
        "    # RAG prompt template\n",
        "    \"ragPromptTemplate\": \"\"\"\n",
        "    Give an answer for the `Question` using only the given `Context`. Use information relevant to the query from the entire context.\n",
        "    Provide a detailed answer with thorough explanations, avoiding summaries.\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "trX6Tl23lyWq"
      },
      "id": "trX6Tl23lyWq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Class\n",
        "`FilingsPreProcessor` (Custom-defined class):\n",
        "A custom class that downloads and preprocesses company filing documents from the web, splitting large filings into smaller text chunks for easier embedding and retrieval.\n",
        "\n",
        "`__init __`:\n",
        "Constructor method that initializes the preprocessor with configuration parameters for chunk size, overlap, and an internal dictionary to store text chunks used specificially for extracting director names.\n",
        "\n",
        "`loadAndProcessFilings`:\n",
        "Method that takes company filing URLs, downloads content, processes it into chunks, and stores the last 1000 characters for director names; returns processed documents and director name chunks.\n",
        "\n",
        "- `Document` (Langchain class):\n",
        "LangChain class representing a document with content and metadata, which is company name in this exercise.\n",
        "\n",
        "- `WebBaseLoader` (Langchain class):\n",
        "  * LangChain class that downloads content from the given 10-K company filing URLs with a custom User-Agent Header, which informs SEC website where the request is coming from and what kind of user agent it is. It's like introducing yourself when you enter a room, but for computer programs! This is often required for accessing web data, especially in responsible web scraping.\n",
        "\n",
        "- `RecursiveCharacterTextSplitter` (Langchain class):\n",
        "LangChain class that splits text documents into smaller chunks with specified size and overlap. It adds the company names as metadata for each chunk, then returns the processed chunks."
      ],
      "metadata": {
        "id": "LjODOSh_qNDo"
      },
      "id": "LjODOSh_qNDo"
    },
    {
      "cell_type": "code",
      "source": [
        "class FilingsPreProcessor:\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.chunkSize = config[\"chunkSize\"]\n",
        "        self.chunkOverlap = config[\"chunkOverlap\"]\n",
        "        self.userAgentHeader = config[\"userAgentHeader\"]\n",
        "\n",
        "    # Using langchain class Document\n",
        "    def loadAndProcessFilings(self, companyFilingUrls: List[Tuple[str, str]]) -> List[Document]:\n",
        "        processedCompanyFilings = []\n",
        "\n",
        "        for company, url in companyFilingUrls:\n",
        "            try:\n",
        "                # Using langchain class WebBaseLoader\n",
        "                filingContent = WebBaseLoader(url, header_template = {'User-Agent': self.userAgentHeader}).load()\n",
        "\n",
        "                # Using langchain class RecursiveCharacterTextSplitter\n",
        "                splitFilingContent = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size = self.chunkSize,\n",
        "                    chunk_overlap = self.chunkOverlap\n",
        "                    ).transform_documents(filingContent)\n",
        "\n",
        "                for split in splitFilingContent:\n",
        "                    split.metadata.update({\n",
        "                        'company': company,\n",
        "                    })\n",
        "\n",
        "                processedCompanyFilings.extend(splitFilingContent)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return processedCompanyFilings"
      ],
      "metadata": {
        "id": "cTPHkr2ssOMH"
      },
      "id": "cTPHkr2ssOMH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding class\n",
        "`EmbeddingService` (Custom-defined class):\n",
        "A custom class that inherits from LangChain's Embeddings class, providing text embedding functionality through an external API\n",
        "\n",
        "`__init __`:\n",
        "Constructor that initializes the service with API URL, headers, and embedding configuration\n",
        "- Sets up the Hugging Face embedding API endpoint an headers\n",
        "- Stores an instruction prefix that is added to user's query before embedding generation.\n",
        "- Sets the embedding dimension size as per `embeddingDim` value in `defaultConfig` (e.g., 768)\n",
        "\n",
        "`embed_documents`:\n",
        "From LangChain's interface - transforms a list of texts into embeddings by removing newlines and calling _generateEmbeddings\n",
        "\n",
        "`embed_query`:\n",
        "From LangChain's interface - transforms a single query text into an embedding by adding instruction prefix and removing newlines\n",
        "\n",
        "`_generateEmbeddings`:\n",
        "Internal helper method that:\n",
        "- Splits the input texts into batches (default size 30)\n",
        "- Sends each batch to the embedding API with _sendBatchRequest\n",
        "- Handles retries and times out if requests take too long\n",
        "- Ensures each embedding matches the expected dimension (e.g., 768)\n",
        "- Returns a list of embeddings\n",
        "\n",
        "`_sendBatchRequest`:\n",
        "Internal helper method that makes POST requests to the embedding API and handles the response"
      ],
      "metadata": {
        "id": "0nutwjAuqTC4"
      },
      "id": "0nutwjAuqTC4"
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingService(Embeddings):               # Using langchain class Embeddings\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.api_url = config[\"embeddingApiUrl\"]\n",
        "        self.headers = {\"Authorization\": f\"Bearer {hfToken}\"}\n",
        "        self.queryInstruction = \"Represent this question for searching relevant passages: \"\n",
        "        self.embeddingDim = config[\"embeddingDim\"]\n",
        "\n",
        "    def embed_documents(self, texts: list[str]) -> List[List[float]]:\n",
        "        textsToEmbed = [text.replace(\"\\n\", \" \") for text in texts]\n",
        "        return self._generateEmbeddings(textsToEmbed)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        textToEmbed = self.queryInstruction + text.replace(\"\\n\", \" \")\n",
        "        return self._generateEmbeddings([textToEmbed])[0]\n",
        "\n",
        "    def _generateEmbeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        results = [np.zeros(self.embeddingDim).tolist() for _ in range(len(texts))]\n",
        "        batchSize = 30\n",
        "        longRetries = 0\n",
        "\n",
        "        batches = [(i, texts[i:i + batchSize]) for i in range(0, len(texts), batchSize)]\n",
        "\n",
        "        for Id, batch in batches:\n",
        "            startTime = time.time()\n",
        "\n",
        "            while time.time() - startTime < 30:\n",
        "                try:\n",
        "                    response = self._sendBatchRequest(batch)\n",
        "\n",
        "                    if not isinstance(response, list):\n",
        "                        continue\n",
        "\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    time.sleep(1)\n",
        "                    continue\n",
        "\n",
        "            if time.time() - startTime > 10:\n",
        "                longRetries += 1\n",
        "                if longRetries > 3:\n",
        "                    print(\"Too many long retries, stopping embedding generation\")\n",
        "                    return results\n",
        "\n",
        "            for j, embedding in enumerate(response):\n",
        "                if isinstance(embedding, list) and len(embedding) == self.embeddingDim:\n",
        "                    results[Id + j] = embedding\n",
        "                else:\n",
        "                    print(f\"Invalid embedding format at index {Id + j}: {embedding[:100]}...\")\n",
        "\n",
        "        if len(texts) > 1:\n",
        "            successful = sum(1 for emb in results if any(emb))\n",
        "            print(f\"Successfully embedded {successful}/{len(texts)} texts\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _sendBatchRequest(self, batch: List[str]) -> List[List[float]]:\n",
        "        response = requests.post(\n",
        "            self.api_url,\n",
        "            headers = self.headers,\n",
        "            json = {\"inputs\": batch}\n",
        "        )\n",
        "        if response.status_code != 200:\n",
        "            raise ValueError(f\"API returned status code {response.status_code}\")\n",
        "        return response.json()"
      ],
      "metadata": {
        "id": "hLSnWAK5xBCl"
      },
      "id": "hLSnWAK5xBCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector DB class\n",
        "`VectorStore` (Custom Class):\n",
        "This custom class manages a FAISS-based vector store for storing and retrieving them by cosine similarity.\n",
        "\n",
        "`__init __`:\n",
        "Constructor that initializes a FAISS vector store with a default document and configures a retriever with number of documents to retrieve.\n",
        "- `FAISS` (Langchain class):\n",
        "A LangChain wrapper class for Facebook AI Similarity Search library. It manages vector database creation, indexes embeddings and retrieves embeddings similar to the queried embedding. Its `from_documents` function accepts a chunk/query, and an instance of `Embedding` Langchain class for generating embeddings for provided chunk/query.\n",
        "- The VectorStore class creates a retriever using self.store.as_retriever(). This retriever is responsible for fetching relevant documents from the vector store based on a query's embedding.\n",
        "\n",
        "`addDocuments`:\n",
        "It adds new documents to the FAISS vector store created in `__init__`, accepting a list of Document objects as input\n",
        "- `Document` (Langchain class):\n",
        " A LangChain document class is used to represent each chunk as a `Document` object.\n",
        "\n"
      ],
      "metadata": {
        "id": "ksNBSQs0qWyf"
      },
      "id": "ksNBSQs0qWyf"
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "    def __init__(self, config, embeddingFunction):\n",
        "        # Using langchain class FAISS\n",
        "        self.store = FAISS.from_documents([config[\"defaultDocument\"]], embeddingFunction)\n",
        "        self.retriever = self.store.as_retriever(\n",
        "            search_kwargs = {\"k\": config[\"numRetrievedDocuments\"]}\n",
        "        )\n",
        "\n",
        "    def addDocuments(self,  documents: List[Document]) -> None:\n",
        "        self.store.add_documents(documents)"
      ],
      "metadata": {
        "id": "hEg0P2yA0FWA"
      },
      "id": "hEg0P2yA0FWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG chain class\n",
        "`RAGChain` (Custom-defined class):\n",
        " A custom class implementing a Retrieval-Augmented Generation (RAG) system that combines document retrieval with language model generation by retrieving the most relevant documents and using them to generate and answer.\n",
        "\n",
        "`__init __`:\n",
        "Initializes the RAG chain with configuration and a vector store retriever\n",
        "\n",
        "- Stores a reference to the `baseRetriever` (from the VectorStore class)\n",
        "- `ChatOpenAI` (Langchain class):\n",
        " LangChain class for interfacing with OpenAI's chat models\n",
        "- prompt is an instance of `PromptTemplate` (a LangChain class) to build the RAG prompt.\n",
        "- Sets the number of documents to be selected from the vector store\n",
        "- Builds the entire chain pipeline via `_makeChain()`\n",
        "\n",
        "Q) Why did we use Langchain's `PromptTemplate` class instead of a string template directly?\n",
        "- It ensures dynamic input substitution with validation, reducing the risk of formatting errors or missing placeholders.\n",
        "- It integrates seamlessly with LangChain's modular components, enhancing composability in workflows and simplifying maintenance.\n",
        "\n",
        "`query`:\n",
        "The main interface method that processes a question through the entire RAG pipeline and returns the LLM-generated answer.\n",
        "\n",
        "`_retrieveContext`:\n",
        "- Retrieves relevant chunks for a given question using the base retriever from VectorStore class\n",
        "\n",
        "- Uses `_formatDocs` to format retreived chunks into a string that can be used as context for the language model. `_formatDocs` does the following:\n",
        "  * Selects only the first `self.numSelectedDocuments` chunks retrieved from vector store.\n",
        "  *  `_getPrefix(doc)` extracts the company name added as metadata to each chunk in `loadAndProcessFilings`, and concatenates it to the text content of each chunk (`doc.page_content`).\n",
        "  * Joins all the chunks into a single string.  It adds 2 line breaks `\\n\\n` as a seperator between two chunks.\n",
        "\n",
        "`_makeChain`:\n",
        "Constructs the processing pipeline using LangChain's Runnable interface components.\n",
        "\n",
        "- `RunnableLambda` is a LangChain utility that wraps a callable Python function (in this case, _retrieveContext) and makes it compatible with LangChain pipelines. The output of this step is the retrieved chunks combined into a string generated by `retrieveContext`.\n",
        "- `RunnablePassthrough` is a LangChain utility that simply passes the user's question to the next step in the pipeline without modifying it. This is useful when you need to include the question alongside the context in a prompt for generating response.\n",
        "- Next step in pipeline replaces placeholder variables {context} and {question} in prompt instance with the retrieved context from `RunnableLambda` and the original question from `RunnablePassthrough`.\n",
        "- `ChatOpenAI` is a LangChain interface for interacting with OpenAI's chat models. This component invokes the OpenAI language model (e.g., GPT-4) to generate a response based on the prompt.\n",
        "- `StrOutputParser` is a LangChain utility that parses the raw output of the LLM (such as the model used via ChatOpenAI) into a clean string format."
      ],
      "metadata": {
        "id": "ZqH_ncryqf9R"
      },
      "id": "ZqH_ncryqf9R"
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGChain:\n",
        "    def __init__(self, config: Dict[str, Any], baseRetriever: VectorStore):\n",
        "        self.baseRetriever = baseRetriever\n",
        "        self.llm = ChatOpenAI(model=config[\"ragAnswerModel\"], temperature=config[\"ragAnswerModelTemperature\"])\n",
        "        self.prompt = PromptTemplate.from_template(config[\"ragPromptTemplate\"])\n",
        "        self.numSelectedDocuments = config[\"numSelectedDocuments\"]\n",
        "\n",
        "    def _retrieveContext(self, question: str) -> str:\n",
        "        retrievedDocs = self.baseRetriever.invoke(question)\n",
        "        return self._formatDocs(retrievedDocs)\n",
        "\n",
        "    def _formatDocs(self, docs: List[Document]) -> str:\n",
        "        def _getPrefix(doc):\n",
        "            company = doc.__dict__.get('metadata', {}).get('company', '')\n",
        "            return company\n",
        "\n",
        "        return \"\\n\\n\".join(f\"{_getPrefix(doc)}\\n{doc.page_content}\" for doc in docs[:self.numSelectedDocuments])\n",
        "\n",
        "    def _makeChain(self):\n",
        "        return (\n",
        "            {\n",
        "                \"context\": RunnableLambda(self._retrieveContext),\n",
        "                \"question\": RunnablePassthrough()\n",
        "            }\n",
        "            | self.prompt\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        return self._makeChain().invoke(question)"
      ],
      "metadata": {
        "id": "-E5-w5iF2Y6X"
      },
      "id": "-E5-w5iF2Y6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing Data\n",
        "\n",
        "So far, we have defined the classes (blueprint) needed for implementing a simple RAG prototype on 10-K filings. These classes provide the foundational structure for tasks like data retrieval and processing. Next, we will create instances to operationalize these tasks and demonstrate preprocessing functionality."
      ],
      "metadata": {
        "id": "BgWBZlOwV38C"
      },
      "id": "BgWBZlOwV38C"
    },
    {
      "cell_type": "code",
      "source": [
        "config = defaultConfig.copy() # Creates a separate copy of the default configuration dictionary (defaultConfig) so that any subsequent changes won't alter the original default settings."
      ],
      "metadata": {
        "id": "i7vrk_fW3Tox"
      },
      "id": "i7vrk_fW3Tox",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking\n",
        "* Creates a `FilingsPreProcessor` instance, responsible for downloading webpages and splitting filings into smaller, more manageable chunks.\n",
        "* Uses the `loadAndProcessFilings` to load, split, and process the filing URLs listed in config[\"companyFilingUrls\"].\n",
        "* Returns chunks ready for indexing in Vector DB.\n"
      ],
      "metadata": {
        "id": "mgIBjW7LVcb6"
      },
      "id": "mgIBjW7LVcb6"
    },
    {
      "cell_type": "code",
      "source": [
        "preProcessorObj = FilingsPreProcessor(config)\n",
        "processedFilings = preProcessorObj.loadAndProcessFilings(config[\"companyFilingUrls\"])"
      ],
      "metadata": {
        "id": "mWWhcqSR4Wly"
      },
      "id": "mWWhcqSR4Wly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding and Vector Storage\n",
        "- Instantiates an `EmbeddingService` object, which provides text embedding functionality (via the Hugging Face Inference API in this example).\n",
        "- Instantiates a `VectorStore` object\n",
        "- Builds a VectorStore object that internally uses a FAISS store to index and retrieve documents based on vector similarity.\n",
        "- Uses embedding object created earlier to generate embeddings of chunks added by `addDocuments`.\n"
      ],
      "metadata": {
        "id": "9NrBQlxCS8yX"
      },
      "id": "9NrBQlxCS8yX"
    },
    {
      "cell_type": "code",
      "source": [
        "embeddingServiceObj = EmbeddingService(config)\n",
        "vectorStoreObj = VectorStore(config, embeddingServiceObj)\n",
        "vectorStoreObj.addDocuments(processedFilings)"
      ],
      "metadata": {
        "id": "h2Hq0bzP32tb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73a8131-94ab-4b21-f2ea-fb66b5e4538e"
      },
      "id": "h2Hq0bzP32tb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully embedded 942/942 texts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Query Processing"
      ],
      "metadata": {
        "id": "0ej7sUEUkQ_I"
      },
      "id": "0ej7sUEUkQ_I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Chain pipeline\n",
        "This pipeline assembles all the necessary pieces:\n",
        "- Takes the configuration settings (`config`).\n",
        "- Connects the RAG pipeline to the vector store through the retriever (`vectorStoreObj.retriever`) and fetches the most relevant chunks based on the user query passed to `ragObj.query`.\n",
        "- Packages the entire pipeline into a single object (`ragObj`) that represents your complete RAG system. `ragObj` is an instance of the `RAGChain`class. You can think of it like a container or a handle that is used to interact with the entire RAG system."
      ],
      "metadata": {
        "id": "Cf1Ir62dnY2o"
      },
      "id": "Cf1Ir62dnY2o"
    },
    {
      "cell_type": "code",
      "source": [
        "ragObj = RAGChain(config, vectorStoreObj.retriever)"
      ],
      "metadata": {
        "id": "xGEyiCSn39F1"
      },
      "id": "xGEyiCSn39F1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pass User Query\n",
        "This is where you pass the user's query to RAG pipeline."
      ],
      "metadata": {
        "id": "VKRGIHSgn0_U"
      },
      "id": "VKRGIHSgn0_U"
    },
    {
      "cell_type": "code",
      "source": [
        "print(ragObj.query(\"What technological advancements were made in the batteries used in Tesla's Electric Vehicles?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja1y5C6UACq-",
        "outputId": "652f765c-9a6b-4d58-cfd8-03e12d2e6a23"
      },
      "id": "ja1y5C6UACq-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla has made significant technological advancements in the batteries used in its electric vehicles (EVs) by focusing on both the development of new battery cells and the enhancement of manufacturing processes. The company has developed a proprietary lithium-ion battery cell, which is aimed at achieving higher energy density while reducing costs. This advancement is crucial as it allows Tesla to produce batteries that can store more energy in the same space, thereby improving the range and efficiency of their EVs.\n",
            "\n",
            "Furthermore, Tesla has invested heavily in research and development (R&D) capabilities for battery cells, packs, and systems. This includes extensive knowledge of lithium-ion cell chemistry types and their performance characteristics, which helps in optimizing the energy density and overall performance of the batteries.\n",
            "\n",
            "In addition to cell development, Tesla has also improved its battery manufacturing processes. These improvements are likely aimed at increasing production efficiency and reducing costs, which are critical factors for scaling up production and making electric vehicles more affordable to a broader market.\n",
            "\n",
            "Tesla's advancements are not limited to the hardware of the batteries. The company also develops sophisticated control software for its vehicles, which plays a crucial role in optimizing battery performance. This software manages charging, enhances safety, and customizes vehicle behavior, ensuring that the battery packs operate at their best possible efficiency and safety levels.\n",
            "\n",
            "Overall, Tesla's advancements in battery technology are centered around increasing energy density, reducing costs, and improving manufacturing processes, all of which contribute to better performing, more affordable electric vehicles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Response Analysis\n",
        "The final response lists the following advancments:\n",
        "\n",
        "\n",
        "1.  Development of a new proprietary lithium-ion battery cell\n",
        "2.  Improved manufacturing processes for the batteries.\n",
        "\n",
        "The response explains these developments and how they impact Tesla.\n",
        "\n",
        "You can see how RAG is able to answer basic queries that by retriving the information required."
      ],
      "metadata": {
        "id": "I8Z5twBQfK3Z"
      },
      "id": "I8Z5twBQfK3Z"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}