{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to Run this Notebook?\n",
        "\n",
        "This notebook builds upon the notebook for Exercise 1: RAG with noReranker. The explanations that repeat from the previous notebook are greyed out in this notebook.\n",
        "\n",
        "<font color=\"#d3d3d3\">\n",
        "1. Generate API key for OpenAI (ChatGPT): https://platform.openai.com/settings/organization/api-keys\n",
        "Make sure to save the API key. You'll get to see the key only once at the time of generation. If you miss copying the key, you may need to generate a new key.<br>\n",
        "2. Generate API token for HuggingFace Source: https://huggingface.co/settings/tokens/new?tokenType=write\n",
        "Just like last time, make sure to save this API key as well.\n",
        "</font>\n",
        "\n",
        "3. Generate API key for Cohere: https://dashboard.cohere.com/api-keys\n",
        "  Look out for `Create API Key` on the top right side of the settings on Cohere website.\n",
        "5. Click on ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADoAAAAkCAIAAABXBb6UAAADK0lEQVRYCe2Xz0vrQBDH/VezS2hUgjk09FJFEX+AB8VICMaTBS+FQgXRg4egYjw09CRY8SRBkcbQYggtsbsPXBj2JbE0zYvvPegcwnR2M/PpN7MTMkf/K5v7r2jpDLfI5zVTd6YuKDB9MwRBYNu2aZr6l5mmadt2EASQughnGtwgCA4PDzHGQsIwxoZh9Hq9IlgpzT7I2u22LMsJzt8Csiy32+0iiLOpe3d3J0kSoGGMK5XKwcGBruvValUURVgSRfH6+vqPE2fAdV13aWmJASGEDMPo9/s8UBiGJycn0CSLi4uPj4/8hvz+pLifn5+7u7uMVRTF09NTQkiyPCHEsiyQeX5+XsluZ2dnycwsMilup9OBNjg6OkplZRkJIbVajf2x6a71ej0vbr1eZ7UVRXl5eWHp3t/fd3Z20Jetrq52u10W931fVdXpWAVByItLCNnb22PlNU1j0g4Gg+3tbZ5pfX0d5u7FxYWexTRNg4GTFzcMw7W1NUYGuZ6enhYWFnhcjLHjOEzgrNfUEskkE/Vuaq77+/tSqcTjCoJg23ayxiSR1BLJGyfCHQ6HW1tbjOz4+JhlSTaoLMvPz89s9ePjw/vewjBk26Io8n3f8zzXdVdWVmIPcEpcSqlpmizX8vIyNOjNzQ0ILIri5eUlKzAYDDY2NmLC8z+ho/iBAxtgdXrc29tbhJAgCBhjy7Igked5V1dXlmW9vb1B0HEcGL0AwTsAVBRuv9+vVqusZLlcdl0X4GLO6+truVzm4ZJ+4biUUsuy4AWrKMrDw0MMlFLqum6lUmF8CKHz83No4NTuBHUlSWq1WmwzdHYy/0RHjd0WRZGu6yAVxnhzc9O27W6363me4zj7+/t8D+i6HkURlOTPfq1WY2StVou9LCVJ6nQ6sPk7JwMupTSKIsMwWBMDd9JBCGmaFhOJx03eUggupXQ0GjWbTRgIycKlUqnZbI5Go5hCfweXQQRB0Gg0VFUFpRFCqqo2Gg0Yc/8QLo8ShuFwOOQjqT4hpNfrwcmLOb7v842emmGaj5/vEv1MPNtR+xmmMVVmuGPEyb00Uze3hGMSzNQdI07upV9MOsjO+LCzRgAAAABJRU5ErkJggg==) icon in the left menu bar of this Notebook\n",
        "\n",
        "7. Click `+ Add new secret `\n",
        "- <font color=\"#d3d3d3\">Add OpenAI key, if not added already:\n",
        "  * <font color=\"#d3d3d3\">Under Name copy paste: `OPENAI_KEY`\n",
        "  * Under Value copy paste: OpenAI key you saved earlier\n",
        "\n",
        "- <font color=\"#d3d3d3\">Add HuggingFace key, if not added already:\n",
        "  * <font color=\"#d3d3d3\">Under Name copy paste: `HF_KEY`\n",
        "  * Under Value copy paste: HF key you saved earlier\n",
        "\n",
        "- Add Cohere key:\n",
        "  * Under Name copy paste: `COHERE_KEY`\n",
        "  * Under Value copy paste: Cohere key you saved earlier\n",
        "\n",
        "8. Enable access to the keys for this notebook by toggling the radio buttons in the `Secrets` section.\n",
        "9. Close the `Secrets` section once done.\n",
        "10. Click `Run all` under the `Runtime` in top menu to execute this notebook`\n"
      ],
      "metadata": {
        "id": "BeskM2NPo-AF"
      },
      "id": "BeskM2NPo-AF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Setup\n",
        "**Install Frameworks**\n",
        "\n",
        "`langchain`, `langchain_core`, and `langchain_community` are components of the LangChain framework.\n",
        "\n",
        "`faiss-cpu` is Facebook AI's similarity search library, optimized for CPU usage.\n",
        "\n",
        "`openai==1.56.2` and `langchain_openai` are OpenAI's Python client library and LangChain's OpenAI integration package respectively.\n",
        "\n",
        "`Cohere` and `langchain_Cohere` are Cohere's Python client library and LangChain's Cohere integration package respectively."
      ],
      "metadata": {
        "id": "a-jK0idCyWtt"
      },
      "id": "a-jK0idCyWtt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8f8207",
      "metadata": {
        "id": "5e8f8207"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_core langchain_community faiss-cpu openai==1.56.2 langchain_openai cohere langchain_cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**\n",
        "\n"
      ],
      "metadata": {
        "id": "uq9Q1b21Ufk-"
      },
      "id": "uq9Q1b21Ufk-"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import requests\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from urllib.parse import urljoin"
      ],
      "metadata": {
        "id": "k1XadP0546aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd2721f-0d79-48b3-81f9-94e2b1a85e34"
      },
      "id": "k1XadP0546aj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "082f85f0",
      "metadata": {
        "id": "082f85f0"
      },
      "source": [
        "**Project's Secret Keys Setup**\n",
        "\n",
        "<font color=\"#d3d3d3\">\n",
        "This code sets up authentication credentials by retrieving API keys from Colab's `Secrets` storage.\n",
        "</font><br>\n",
        "Cohere's API key is also retreieved from Colab's `Secrets` storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da18250",
      "metadata": {
        "id": "1da18250"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_KEY')\n",
        "hfToken = userdata.get('HF_KEY')\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration Dictionary\n",
        "<font color=\"#d3d3d3\">\n",
        "Contains settings for document processing and RAG system, including parameters for text chunking, embedding generation, document retrieval, model specifications, prompts used and API endpoints.</font><br>\n",
        "Default config settings for Cohere's Reranker were added."
      ],
      "metadata": {
        "id": "2k78A3s4ExZq"
      },
      "id": "2k78A3s4ExZq"
    },
    {
      "cell_type": "code",
      "source": [
        "defaultConfig = {\n",
        "    # Document processing settings\n",
        "    \"chunkSize\": 500,\n",
        "    \"chunkOverlap\": 50,\n",
        "    \"userAgentHeader\": \"YourCompany-ResearchBot/1.0 (your@email.com)\",\n",
        "\n",
        "    # Embedding settings\n",
        "    \"embeddingApiUrl\": \"https://api-inference.huggingface.co/models/BAAI/bge-base-en-v1.5\",\n",
        "    \"embeddingDim\": 768,\n",
        "\n",
        "    #Defalt document\n",
        "    \"defaultDocument\" : Document(\n",
        "        page_content = \"This is a default document.\",\n",
        "        metadata = {\"source\": \"default\"}\n",
        "    ),\n",
        "\n",
        "    # Vector store settings\n",
        "    \"numRetrievedDocuments\": 12,\n",
        "\n",
        "    # Document formatter settings\n",
        "    \"numSelectedDocuments\": 5,\n",
        "\n",
        "    #Reranker setting\n",
        "    \"rerankerModel\": \"rerank-english-v3.0\",\n",
        "    \"numRerankedDocuments\": 5,\n",
        "\n",
        "    # Model settings\n",
        "    \"ragAnswerModel\": \"gpt-4o\",\n",
        "    \"ragAnswerModelTemeprature\": 0.7,\n",
        "\n",
        "    # URLs to process\n",
        "    \"companyFilingUrls\": [\n",
        "        (\"Tesla\", \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\")\n",
        "    ],\n",
        "\n",
        "    # RAG prompt template\n",
        "    \"ragPromptTemplate\": \"\"\"\n",
        "    Give an answer for the `Question` using only the given `Context`. Use information relevant to the query from the entire context.\n",
        "    Provide a detailed answer with thorough explanations, avoiding summaries.\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "trX6Tl23lyWq"
      },
      "id": "trX6Tl23lyWq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Class\n",
        "<font color=\"#d3d3d3\">`FilingsPreProcessor` (Custom-defined class):\n",
        "A custom class that downloads and preprocesses company filing documents from the web, splitting large filings into smaller text chunks for easier embedding and retrieval.<br>\n",
        "`__init __`:\n",
        "Constructor method that initializes the preprocessor with configuration parameters for chunk size, overlap, and an internal dictionary to store text chunks used specificially for extracting director names.<br>\n",
        "`loadAndProcessFilings`:\n",
        "Method that takes company filing URLs, downloads content, processes it into chunks, and stores the last 1000 characters for director names; returns processed documents and director name chunks.\n",
        "\n",
        "- <font color=\"#d3d3d3\">`Document` (Langchain class):\n",
        "LangChain class representing a document with content and metadata, which is company name in this exercise.\n",
        "\n",
        "- <font color=\"#d3d3d3\">`WebBaseLoader` (Langchain class):\n",
        "  * <font color=\"#d3d3d3\">LangChain class that downloads content from the given 10-K company filing URLs with a custom User-Agent Header, which informs SEC website where the request is coming from and what kind of user agent it is. It's like introducing yourself when you enter a room, but for computer programs! This is often required for accessing web data, especially in responsible web scraping.\n",
        "\n",
        "- <font color=\"#d3d3d3\">`RecursiveCharacterTextSplitter` (Langchain class):\n",
        "LangChain class that splits text documents into smaller chunks with specified size and overlap. It adds the company names as metadata for each chunk, then returns the processed chunks.\n",
        "</font>"
      ],
      "metadata": {
        "id": "LjODOSh_qNDo"
      },
      "id": "LjODOSh_qNDo"
    },
    {
      "cell_type": "code",
      "source": [
        "class FilingsPreProcessor:\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.chunkSize = config[\"chunkSize\"]\n",
        "        self.chunkOverlap = config[\"chunkOverlap\"]\n",
        "        self.userAgentHeader = config[\"userAgentHeader\"]\n",
        "\n",
        "    # Using langchain class Document\n",
        "    def loadAndProcessFilings(self, companyFilingUrls: List[Tuple[str, str]]) -> List[Document]:\n",
        "        processedCompanyFilings = []\n",
        "\n",
        "        for company, url in companyFilingUrls:\n",
        "            try:\n",
        "                # Using langchain class WebBaseLoader\n",
        "                filingContent = WebBaseLoader(url, header_template = {'User-Agent': self.userAgentHeader}).load()\n",
        "\n",
        "                # Using langchain class RecursiveCharacterTextSplitter\n",
        "                splitFilingContent = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size = self.chunkSize,\n",
        "                    chunk_overlap = self.chunkOverlap\n",
        "                    ).transform_documents(filingContent)\n",
        "\n",
        "                for split in splitFilingContent:\n",
        "                    split.metadata.update({\n",
        "                        'company': company,\n",
        "                    })\n",
        "\n",
        "                processedCompanyFilings.extend(splitFilingContent)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return processedCompanyFilings"
      ],
      "metadata": {
        "id": "cTPHkr2ssOMH"
      },
      "id": "cTPHkr2ssOMH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding class\n",
        "<font color=\"#d3d3d3\">`EmbeddingService` (Custom-defined class):\n",
        "A custom class that inherits from LangChain's Embeddings class, providing text embedding functionality through an external API<br>\n",
        "`__init __`:\n",
        "Constructor that initializes the service with API URL, headers, and embedding configuration\n",
        "- <font color=\"#d3d3d3\">Sets up the Hugging Face embedding API endpoint an headers\n",
        "- Stores an instruction prefix that is added to user's query before embedding generation.\n",
        "- Sets the embedding dimension size as per `embeddingDim` value in `defaultConfig` (e.g., 768)\n",
        "\n",
        "<font color=\"#d3d3d3\">`embed_documents`:\n",
        "From LangChain's interface - transforms a list of texts into embeddings by removing newlines and calling _generateEmbeddings\n",
        "\n",
        "<font color=\"#d3d3d3\">`embed_query`:\n",
        "From LangChain's interface - transforms a single query text into an embedding by adding instruction prefix and removing newlines\n",
        "\n",
        "<font color=\"#d3d3d3\">`_generateEmbeddings`:\n",
        "Internal helper method that:\n",
        "- <font color=\"#d3d3d3\">Splits the input texts into batches (default size 30)\n",
        "- Sends each batch to the embedding API with _sendBatchRequest\n",
        "- Handles retries and times out if requests take too long\n",
        "- Ensures each embedding matches the expected dimension (e.g., 768)\n",
        "- Returns a list of embeddings\n",
        "\n",
        "<font color=\"#d3d3d3\">`_sendBatchRequest`:\n",
        "Internal helper method that makes POST requests to the embedding API and handles the response"
      ],
      "metadata": {
        "id": "0nutwjAuqTC4"
      },
      "id": "0nutwjAuqTC4"
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingService(Embeddings):               # Using langchain class Embeddings\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.api_url = config[\"embeddingApiUrl\"]\n",
        "        self.headers = {\"Authorization\": f\"Bearer {hfToken}\"}\n",
        "        self.queryInstruction = \"Represent this question for searching relevant passages: \"\n",
        "        self.embeddingDim = config[\"embeddingDim\"]\n",
        "\n",
        "    def embed_documents(self, texts: list[str]) -> List[List[float]]:\n",
        "        textsToEmbed = [text.replace(\"\\n\", \" \") for text in texts]\n",
        "        return self._generateEmbeddings(textsToEmbed)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        textToEmbed = self.queryInstruction + text.replace(\"\\n\", \" \")\n",
        "        return self._generateEmbeddings([textToEmbed])[0]\n",
        "\n",
        "    def _generateEmbeddings(self, texts: List[str]) -> List[List[float]]:\n",
        "        results = [np.zeros(self.embeddingDim).tolist() for _ in range(len(texts))]\n",
        "        batchSize = 30\n",
        "        longRetries = 0\n",
        "\n",
        "        batches = [(i, texts[i:i + batchSize]) for i in range(0, len(texts), batchSize)]\n",
        "\n",
        "        for Id, batch in batches:\n",
        "            startTime = time.time()\n",
        "\n",
        "            while time.time() - startTime < 30:\n",
        "                try:\n",
        "                    response = self._sendBatchRequest(batch)\n",
        "\n",
        "                    if not isinstance(response, list):\n",
        "                        continue\n",
        "\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    time.sleep(1)\n",
        "                    continue\n",
        "\n",
        "            if time.time() - startTime > 10:\n",
        "                longRetries += 1\n",
        "                if longRetries > 3:\n",
        "                    print(\"Too many long retries, stopping embedding generation\")\n",
        "                    return results\n",
        "\n",
        "            for j, embedding in enumerate(response):\n",
        "                if isinstance(embedding, list) and len(embedding) == self.embeddingDim:\n",
        "                    results[Id + j] = embedding\n",
        "                else:\n",
        "                    print(f\"Invalid embedding format at index {Id + j}: {embedding[:100]}...\")\n",
        "\n",
        "        if len(texts) > 1:\n",
        "            successful = sum(1 for emb in results if any(emb))\n",
        "            print(f\"Successfully embedded {successful}/{len(texts)} texts\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _sendBatchRequest(self, batch: List[str]) -> List[List[float]]:\n",
        "        response = requests.post(\n",
        "            self.api_url,\n",
        "            headers = self.headers,\n",
        "            json = {\"inputs\": batch}\n",
        "        )\n",
        "        if response.status_code != 200:\n",
        "            raise ValueError(f\"API returned status code {response.status_code}\")\n",
        "        return response.json()"
      ],
      "metadata": {
        "id": "hLSnWAK5xBCl"
      },
      "id": "hLSnWAK5xBCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector DB class\n",
        "<font color=\"#d3d3d3\">`VectorStore` (Custom Class):\n",
        "This custom class manages a FAISS-based vector store for storing and retrieving them by cosine similarity.\n",
        "\n",
        "<font color=\"#d3d3d3\">`__init __`:\n",
        "Constructor that initializes a FAISS vector store with a default document and configures a retriever with number of documents to retrieve.\n",
        "- <font color=\"#d3d3d3\">`FAISS` (Langchain class):\n",
        "A LangChain wrapper class for Facebook AI Similarity Search library. It manages vector database creation, indexes embeddings and retrieves embeddings similar to the queried embedding. Its `from_documents` function accepts a chunk/query, and an instance of `Embedding` Langchain class for generating embeddings for provided chunk/query.\n",
        "- The VectorStore class creates a retriever using self.store.as_retriever(). This retriever is responsible for fetching relevant documents from the vector store based on a query's embedding.\n",
        "\n",
        "<font color=\"#d3d3d3\">`addDocuments`:\n",
        "It adds new documents to the FAISS vector store created in `__init__`, accepting a list of Document objects as input\n",
        "- <font color=\"#d3d3d3\">`Document` (Langchain class):\n",
        " A LangChain document class is used to represent each chunk as a `Document` object.\n",
        "\n"
      ],
      "metadata": {
        "id": "ksNBSQs0qWyf"
      },
      "id": "ksNBSQs0qWyf"
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "    def __init__(self, config, embeddingFunction):\n",
        "        # Using langchain class FAISS\n",
        "        self.store = FAISS.from_documents([config[\"defaultDocument\"]], embeddingFunction)\n",
        "        self.retriever = self.store.as_retriever(\n",
        "            search_kwargs = {\"k\": config[\"numRetrievedDocuments\"]}\n",
        "        )\n",
        "\n",
        "    def addDocuments(self,  documents: List[Document]) -> None:\n",
        "        self.store.add_documents(documents)"
      ],
      "metadata": {
        "id": "hEg0P2yA0FWA"
      },
      "id": "hEg0P2yA0FWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reranker Function\n",
        " It enhances the quality of search results by re-ranking them using Cohere's reranking capabilities within the LangChain framework.\n",
        "\n",
        "`createReranker`:\n",
        "The first parameter in this function is the name of Cohere's Model that we want to use and the default model name is obtained from `config`. `numRerankedDocuments` defines number of top-ranked chunks to keep after reranking.\n",
        "\n",
        "- `CohereRerank` (Langchain class):\n",
        "LangChain class specifically for reranking chunks using Cohere's reranking API. It reorders chunks based on their relevance to a query using Cohere's model\n",
        "\n",
        "- `ContextualCompressionRetriever` (Langchain class):\n",
        "It sets the previously created CohereRerank instance, `compressor`, as `base_compressor`. It also requires a `base_retriever` which is the retriever object retruned from vector database retrieval."
      ],
      "metadata": {
        "id": "MLWpwaAV6Amd"
      },
      "id": "MLWpwaAV6Amd"
    },
    {
      "cell_type": "code",
      "source": [
        "def createReranker(config, baseRetriever):\n",
        "    # Using langchain class CohereRerank\n",
        "    compressor = CohereRerank(model=config[\"rerankerModel\"], top_n = config[\"numRerankedDocuments\"])\n",
        "\n",
        "    # Using langchain class ContextualCompressionRetriever\n",
        "    return ContextualCompressionRetriever(\n",
        "        base_compressor = compressor, base_retriever = baseRetriever\n",
        "    )"
      ],
      "metadata": {
        "id": "E8rVJiaH5_lE"
      },
      "id": "E8rVJiaH5_lE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG chain class\n",
        "<font color=\"#d3d3d3\">`RAGChain` (Custom-defined class):\n",
        " A custom class implementing a Retrieval-Augmented Generation (RAG) system that combines document retrieval with language model generation by retrieving the most relevant documents and using them to generate and answer.\n",
        "\n",
        "<font color=\"#d3d3d3\">`__init __`:\n",
        "Initializes the RAG chain with configuration and a vector store retriever\n",
        "\n",
        "- <font color=\"#d3d3d3\">Stores a reference to the `baseRetriever` (from the VectorStore class)\n",
        "- `ChatOpenAI` (Langchain class):\n",
        " LangChain class for interfacing with OpenAI's chat models\n",
        "- prompt is an instance of `PromptTemplate` (a LangChain class) to build the RAG prompt.\n",
        "- Sets the number of documents to be selected from the vector store</font>\n",
        "- `retrieverWithReranking`: It makes the reranking retriever available for use within the RAGChain instance.\n",
        "- <font color=\"#d3d3d3\">Builds the entire chain pipeline via `_makeChain()`\n",
        "\n",
        "<font color=\"#d3d3d3\">Q) Why did we use Langchain's `PromptTemplate` class instead of a string template directly?\n",
        "- <font color=\"#d3d3d3\">It ensures dynamic input substitution with validation, reducing the risk of formatting errors or missing placeholders.\n",
        "- It integrates seamlessly with LangChain's modular components, enhancing composability in workflows and simplifying maintenance.\n",
        "\n",
        "<font color=\"#d3d3d3\">`query`:\n",
        "The main interface method that processes a question through the entire RAG pipeline and returns the LLM-generated answer.\n",
        "\n",
        "`_retrieveContext`:\n",
        "- if fReranker is false, it retrieves relevant chunks for a given question using the base retriever from `VectorStore` class\n",
        "- else, it retrieves reranked relevant chunks for a given question using the retriever with reranking from `CohereRerank` class\n",
        "- Uses `_formatDocs` to format (reranked) retrieved chunks into a string that can be used as context for the language model. <font color=\"#d3d3d3\">`_formatDocs` does the following:\n",
        "  * <font color=\"#d3d3d3\">Selects only the first `self.numSelectedDocuments` chunks retrieved from vector store.\n",
        "  *  `_getPrefix(doc)` extracts the company name added as metadata to each chunk in `loadAndProcessFilings`, and concatenates it to the text content of each chunk (`doc.page_content`).\n",
        "  * Joins all the chunks into a single string.  It adds 2 line breaks `\\n\\n` as a seperator between two chunks.\n",
        "\n",
        "<font color=\"#d3d3d3\">`_makeChain`:\n",
        "Constructs the processing pipeline using LangChain's Runnable interface components.\n",
        "\n",
        "- <font color=\"#d3d3d3\">`RunnableLambda` is a LangChain utility that wraps a callable Python function (in this case, _retrieveContext) and makes it compatible with LangChain pipelines. The output of this step is the retrieved chunks combined into a string generated by `retrieveContext`.\n",
        "- `RunnablePassthrough` is a LangChain utility that simply passes the user's question to the next step in the pipeline without modifying it. This is useful when you need to include the question alongside the context in a prompt for generating response.\n",
        "- Next step in pipeline replaces placeholder variables {context} and {question} in prompt instance with the retrieved context from `RunnableLambda` and the original question from `RunnablePassthrough`.\n",
        "- `ChatOpenAI` is a LangChain interface for interacting with OpenAI's chat models. This component invokes the OpenAI language model (e.g., GPT-4) to generate a response based on the prompt.\n",
        "- `StrOutputParser` is a LangChain utility that parses the raw output of the LLM (such as the model used via ChatOpenAI) into a clean string format."
      ],
      "metadata": {
        "id": "ZqH_ncryqf9R"
      },
      "id": "ZqH_ncryqf9R"
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGChain:\n",
        "    def __init__(self, config: Dict[str, Any], baseRetriever: VectorStore, fReranker = False):\n",
        "        self.config = config\n",
        "        self.baseRetriever = baseRetriever\n",
        "        self.llm = ChatOpenAI(model=config[\"ragAnswerModel\"], temperature=config[\"ragAnswerModelTemeprature\"])\n",
        "        self.prompt = PromptTemplate.from_template(config[\"ragPromptTemplate\"])\n",
        "        self.numSelectedDocuments = config[\"numSelectedDocuments\"]\n",
        "        self.fReranker = fReranker\n",
        "        if self.fReranker:\n",
        "                self.retrieverWithReranking = createReranker(self.config, self.baseRetriever)\n",
        "\n",
        "    def _retrieveContext(self, question: str) -> str:\n",
        "        if self.fReranker:\n",
        "            rerankedDocs = self.retrieverWithReranking.invoke(question)\n",
        "            return self._formatDocs(rerankedDocs)\n",
        "        else:\n",
        "            retrievedDocs = self.baseRetriever.invoke(question)\n",
        "            return self._formatDocs(retrievedDocs)\n",
        "\n",
        "    def _formatDocs(self, docs: List[Document]) -> str:\n",
        "        def _getPrefix(doc):\n",
        "            company = doc.__dict__.get('metadata', {}).get('company', '')\n",
        "            return company\n",
        "\n",
        "        return \"\\n\\n\".join(f\"{_getPrefix(doc)}\\n{doc.page_content}\" for doc in docs[:self.numSelectedDocuments])\n",
        "\n",
        "    def _makeChain(self):\n",
        "        return (\n",
        "            {\n",
        "                \"context\": RunnableLambda(self._retrieveContext),\n",
        "                \"question\": RunnablePassthrough()\n",
        "            }\n",
        "            | self.prompt\n",
        "            | self.llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        return self._makeChain().invoke(question)"
      ],
      "metadata": {
        "id": "-E5-w5iF2Y6X"
      },
      "id": "-E5-w5iF2Y6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing Data\n",
        "\n",
        "<font color=\"#d3d3d3\">So far, we have defined the classes (blueprint) needed for implementing a simple RAG prototype on 10-K filings. These classes provide the foundational structure for tasks like data retrieval and processing. Next, we will create instances to operationalize these tasks and demonstrate preprocessing functionality."
      ],
      "metadata": {
        "id": "BgWBZlOwV38C"
      },
      "id": "BgWBZlOwV38C"
    },
    {
      "cell_type": "code",
      "source": [
        "config = defaultConfig.copy() # Creates a separate copy of the default configuration dictionary (defaultConfig) so that any subsequent changes won't alter the original default settings."
      ],
      "metadata": {
        "id": "i7vrk_fW3Tox"
      },
      "id": "i7vrk_fW3Tox",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking\n",
        "* <font color=\"#d3d3d3\">Creates a `FilingsPreProcessor` instance, responsible for downloading webpages and splitting filings into smaller, more manageable chunks.\n",
        "* Uses the `loadAndProcessFilings` to load, split, and process the filing URLs listed in config[\"companyFilingUrls\"].\n",
        "* Returns chunks ready for indexing in Vector DB.\n"
      ],
      "metadata": {
        "id": "mgIBjW7LVcb6"
      },
      "id": "mgIBjW7LVcb6"
    },
    {
      "cell_type": "code",
      "source": [
        "preProcessorObj = FilingsPreProcessor(config)\n",
        "processedFilings = preProcessorObj.loadAndProcessFilings(config[\"companyFilingUrls\"])"
      ],
      "metadata": {
        "id": "mWWhcqSR4Wly"
      },
      "id": "mWWhcqSR4Wly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding and Vector Storage\n",
        "- <font color=\"#d3d3d3\">Instantiates an `EmbeddingService` object, which provides text embedding functionality (via the Hugging Face Inference API in this example).\n",
        "- Instantiates a `VectorStore` object\n",
        "- Builds a VectorStore object that internally uses a FAISS store to index and retrieve documents based on vector similarity.\n",
        "- Uses embedding object created earlier to generate embeddings of chunks added by `addDocuments`.\n"
      ],
      "metadata": {
        "id": "9NrBQlxCS8yX"
      },
      "id": "9NrBQlxCS8yX"
    },
    {
      "cell_type": "code",
      "source": [
        "embeddingServiceObj = EmbeddingService(config)\n",
        "vectorStoreObj = VectorStore(config, embeddingServiceObj)\n",
        "vectorStoreObj.addDocuments(processedFilings)"
      ],
      "metadata": {
        "id": "h2Hq0bzP32tb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0271962a-2e4b-48b5-a820-ebd8adcf0e3e"
      },
      "id": "h2Hq0bzP32tb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully embedded 942/942 texts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Query Processing"
      ],
      "metadata": {
        "id": "0ej7sUEUkQ_I"
      },
      "id": "0ej7sUEUkQ_I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Chain pipeline\n",
        "<font color=\"#d3d3d3\">This pipeline assembles all the necessary pieces:\n",
        "- <font color=\"#d3d3d3\">Takes the configuration settings (`config`).\n",
        "- Connects the RAG pipeline to the vector store through the retriever (`vectorStoreObj.retriever`) and fetches the most relevant chunks based on the user query passed to `ragObj.query`.\n",
        "- Packages the entire pipeline into a single object (`ragObj`) that represents your complete RAG system. `ragObj` is an instance of the `RAGChain`class. You can think of it like a container or a handle that is used to interact with the entire RAG system.</font>\n",
        "- Since the `fReranker` value is not defined, its default value of `false` will be used in the `RAGChain`. This means the retrieved chunks will not be processed through the Reranker."
      ],
      "metadata": {
        "id": "Cf1Ir62dnY2o"
      },
      "id": "Cf1Ir62dnY2o"
    },
    {
      "cell_type": "code",
      "source": [
        "ragObj = RAGChain(config, vectorStoreObj.retriever)"
      ],
      "metadata": {
        "id": "xGEyiCSn39F1"
      },
      "id": "xGEyiCSn39F1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only change from the last statement is that `fReranker` value is passed as `true`. This means the retrieved chunks will be processed through the Reranker."
      ],
      "metadata": {
        "id": "89MzbKN1JEMm"
      },
      "id": "89MzbKN1JEMm"
    },
    {
      "cell_type": "code",
      "source": [
        "rerankerRagObj = RAGChain(config, vectorStoreObj.retriever, True)"
      ],
      "metadata": {
        "id": "EUx_jHNoDQkW"
      },
      "id": "EUx_jHNoDQkW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pass User Query\n",
        "This is where you pass the user's query to RAG pipeline.\n",
        "\n",
        "Note: The spelling mistakes in the query are intentional. We use only one 10-K document to ensure that the code runs within a reasonable time in the free Colab version, thereby reducing preprocessing time. The retrieval performs well even without the Reranker for many queries due to the smaller number of chunks. Therefore, to demonstrate the value of the Reranker, we identified a query that consistently underperforms without reranker.\n",
        "\n",
        "In real-world scenarios, datasets are much larger and cosine based similarity often make mistakes in ranking chunks. Hence, a base retriever alone is typically insufficient without a Reranker."
      ],
      "metadata": {
        "id": "VKRGIHSgn0_U"
      },
      "id": "VKRGIHSgn0_U"
    },
    {
      "cell_type": "code",
      "source": [
        "print(ragObj.query(\"List the majour changes that occoured in the company in 2023\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja1y5C6UACq-",
        "outputId": "a694356e-c322-494d-b0fe-e116f7913f95"
      },
      "id": "ja1y5C6UACq-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 2023, Tesla experienced several major changes across various aspects of its operations:\n",
            "\n",
            "1. **Data Security Incident**: In the second quarter of 2023, Tesla faced a significant data security incident where a foreign news outlet reported obtaining misappropriated data, which included purportedly non-public Tesla business and personal information. This event necessitated notifications to potentially affected parties and highlighted ongoing challenges in protecting sensitive company data.\n",
            "\n",
            "2. **Financial and Operational Changes**: Tesla's selling, general, and administrative (SG&A) expenses saw a notable increase by $854 million, or 22%, compared to the previous year. This increase was primarily driven by a $447 million rise in employee and labor costs due to an increase in headcount, including professional services, and a $363 million increase in facilities-related expenses. These financial shifts indicate growth and expansion in Tesla's operations, necessitating more resources and infrastructure.\n",
            "\n",
            "3. **Production Adjustments**: In the third quarter of 2023, Tesla experienced a sequential decline in production volumes. This was due to pre-planned shutdowns at various factories for upgrades. Such shutdowns are part of Tesla's strategy to enhance its manufacturing capabilities and align with ambitious technological goals, particularly in battery technology.\n",
            "\n",
            "These changes reflect both the challenges and strategic initiatives Tesla faced in 2023, highlighting the company's focus on expanding its workforce, enhancing production capabilities, and navigating cybersecurity and operational risks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Response Analysis\n",
        "The final response lists the following advancments:\n",
        "\n",
        "\n",
        "1.  Development of a new proprietary lithium-ion battery cell\n",
        "2.  Improved manufacturing processes for the batteries.\n",
        "\n",
        "The response explains these developments and how they impact Tesla.\n",
        "\n",
        "You can see how RAG is able to answer basic queries that by retrieving the information required."
      ],
      "metadata": {
        "id": "I8Z5twBQfK3Z"
      },
      "id": "I8Z5twBQfK3Z"
    },
    {
      "cell_type": "code",
      "source": [
        "print(rerankerRagObj.query(\"List the majour changes that occoured in the company in 2023\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6aFlJ9n8_sH",
        "outputId": "3d20537b-14a3-4ece-c3c8-ba35f66015e4"
      },
      "id": "L6aFlJ9n8_sH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 2023, Tesla experienced several major changes across various aspects of its operations and financial management:\n",
            "\n",
            "1. **Rule 10b5-1 Trading Arrangement**: In October 2023, Robyn Denholm, a director at Tesla, adopted a Rule 10b5-1 trading arrangement. This arrangement would potentially allow for the sale of up to 281,116 shares of Tesla's common stock, contingent on certain conditions. This type of arrangement is typically used to mitigate insider trading risks by allowing insiders to set up a predetermined plan for selling stocks.\n",
            "\n",
            "2. **Increased SG&A Expenses**: Teslaâ€™s Selling, General, and Administrative (SG&A) expenses increased significantly by $854 million, or 22%, in 2023 compared to 2022. This increase was largely due to a $447 million rise in employee and labor costs, stemming from increased headcount, including professional services, and a $363 million increase in facilities-related expenses. These changes suggest a strategic investment in personnel and infrastructure to support growth.\n",
            "\n",
            "3. **Cash and Cash Equivalents**: By the end of 2023, Tesla's cash and cash equivalents, along with investments, grew to $29.09 billion, which is an increase of $6.91 billion from the end of 2022. This indicates a strong liquidity position, which is critical for funding ongoing and future projects.\n",
            "\n",
            "4. **Operating Cash Flow**: Despite the increase in cash and cash equivalents, Tesla's cash flow from operating activities decreased by $1.47 billion, with operating cash flows at $13.26 billion in 2023 compared to $14.72 billion in 2022. This decline could reflect higher operational costs or changes in working capital.\n",
            "\n",
            "5. **Capital Expenditures**: Tesla's capital expenditures rose to $8.90 billion in 2023, up from $7.16 billion in 2022, marking an increase of $1.74 billion. This suggests significant investment in infrastructure and technology, possibly related to expanding production capacity and upgrading facilities.\n",
            "\n",
            "6. **Production Volumes and Technological Developments**: In the third quarter of 2023, Tesla experienced a sequential decline in production volumes due to pre-planned shutdowns for upgrades at various factories. This strategic decision was likely aimed at implementing new product and manufacturing technologies. Tesla also set ambitious technological goals, particularly concerning battery development, which could substantially impact future production and product offerings.\n",
            "\n",
            "These changes reflect Tesla's ongoing efforts to expand its operational capabilities, enhance its technological advancements, and manage its financial resources effectively to support its long-term growth strategy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without the Reranker, the RAG system failed to retrieve information about a Tesla director, Robyn Denholm, planning to sell up to 281,116 shares of the company's common stock. Many would argue that such a development represents a significant event for Tesla in 2023."
      ],
      "metadata": {
        "id": "RwWXzh1fLZ57"
      },
      "id": "RwWXzh1fLZ57"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}